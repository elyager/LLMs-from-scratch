{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "123g1w8Vz_0T"
   },
   "source": [
    "### Requiriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCSj5qDn9bP-",
    "outputId": "dab1e62e-fbb2-4d93-f4a6-3081485daa0f"
   },
   "outputs": [],
   "source": [
    "# pip install tiktoken\n",
    "# import torch\n",
    "# print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiqznPKxE2-J"
   },
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZQd3ILu5Czb"
   },
   "source": [
    "- **vocab_size:** es el tama√±o del vocabulario en tokens √∫nicos disponibles+extensiones. En nuestro caso dado por el tokenizador creado con BPE.\n",
    "- **output_dim:** el n√∫mero de dimensiones de cada token. Las dimensiones describen a una palabra o concepto. M√°s dimensiones capturan m√°s detalles.\n",
    "- **max_length:** es la m√°xima logitud de tokens por secuencia.\n",
    "- **batch_size:** es cuantas secuencias tiene cada batch.\n",
    "- **stride:** el tama√±o de la zancada en tokens, cuantos tokens salta para la siguiente secuencia, esto determina que tanto se empalma una secuencia con otra.\n",
    "- **shuffle:** determina si le da un orden aleatorio a las secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQyY8us8kjnU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load the text for training (our corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLL4xfSX9C0p",
    "outputId": "d019821e-5f3c-47be-a96d-c76877b76aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdqVKwHw9WOr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BytePair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SpmSakIi9grv",
    "outputId": "b731eaeb-c606-4892-8538-2fdea0a3d732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoAAtvXt9r53",
    "outputId": "47d5bcec-28f0-4e44-fbd0-70d398b27f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I |-| H |-|AD |-| always |-| thought |-| Jack |-| G |-|is |-|burn |-| rather\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n",
      "\n",
      " Total of tokens: 5146\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_text = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "enc_text.append(tokenizer.eot_token)\n",
    "\n",
    "# First 10 tokens from raw_text\n",
    "first_10_token_ids = enc_text[:10]\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in first_10_token_ids]\n",
    "delimited_tokens = ' |-|'.join(decoded_tokens)\n",
    "print(delimited_tokens)\n",
    "print(enc_text[:10])\n",
    "print(f'\\n Total of tokens: {len(enc_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5oaJW523vt2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataset loader (creating tokenIDs for inputs and targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6B2KxAkAbLC"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids.append(tokenizer.eot_token)\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# dataset = GPTDatasetV1(raw_text, tokenizer, max_length=4, stride=1)\n",
    "# print(dataset.input_ids)\n",
    "# print(dataset.target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXlE1wG6AsOk"
   },
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0nYzc5uBMkF"
   },
   "source": [
    "### Use DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gIJnkOxAvfj",
    "outputId": "c3806535-bfe1-4857-f97a-6bb4cc9b033f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     input, target = batch\n",
    "#     print(input, target)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch) # input and target\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch) # input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7Oyu9uJEay2",
    "outputId": "445e55b4-5d47-44d2-9302-907a4ce66c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n",
      "Inputs:\n",
      " I HAD always\n",
      "\n",
      "Targets:\n",
      "  HAD always thought\n",
      "Inputs:\n",
      " tensor([[  287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339],\n",
      "        [  550,  5710,   465, 12036],\n",
      "        [   11,  6405,   257,  5527],\n",
      "        [27075,    11,   290,  4920],\n",
      "        [ 2241,   287,   257,  4489],\n",
      "        [   64,   319,   262, 34686],\n",
      "        [41976,    13,   357, 10915]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  262,  6001,   286,   465],\n",
      "        [13476,    11,   339,   550],\n",
      "        [ 5710,   465, 12036,    11],\n",
      "        [ 6405,   257,  5527, 27075],\n",
      "        [   11,   290,  4920,  2241],\n",
      "        [  287,   257,  4489,    64],\n",
      "        [  319,   262, 34686, 41976],\n",
      "        [   13,   357, 10915,   314]])\n",
      "Inputs:\n",
      "  in the height of\n",
      "\n",
      "Targets:\n",
      "  the height of his\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "# First batch\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
    "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))\n",
    "\n",
    "# Second batch\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
    "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXjmjDeM6z-5"
   },
   "source": [
    "### Create our token embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ax4UNR13E4wB",
    "outputId": "0d75b83a-d936-4b83-9fb8-54f36787c7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 3])\n",
      "Parameter containing:\n",
      "tensor([[-0.3140,  0.3158, -0.0184],\n",
      "        [-0.6753, -0.3501,  0.3179],\n",
      "        [ 0.9742,  0.9866, -0.2060],\n",
      "        ...,\n",
      "        [-0.3815, -0.2799, -1.2723],\n",
      "        [-0.1445, -0.1240,  0.9424],\n",
      "        [ 0.0474,  2.2421,  1.2822]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 3 # 256 is a more common starting point\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(token_embedding_layer.weight.shape)\n",
    "print(token_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaPfzy6d631b"
   },
   "source": [
    "#### Load our dataset to get the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuqLpfanGGBp",
    "outputId": "a73fd08c-57e8-4236-881b-0f9fefb443fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token IDs:\\n\",  inputs) # we take the first batch and  ignore the targets for now\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-qc5coF7P9c"
   },
   "source": [
    "### Create token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ziD4fWmIUMy",
    "outputId": "6ccd37f9-8b94-4d4d-bd5f-df7679770634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 3])\n",
      "tensor([[[-1.5667, -0.4043,  2.0194],\n",
      "         [ 0.0318, -0.5878,  0.1383],\n",
      "         [ 0.1082,  0.3448,  0.5661],\n",
      "         [ 1.2075,  0.2069, -0.4459]],\n",
      "\n",
      "        [[ 0.4012,  0.3681, -0.5622],\n",
      "         [-1.4374, -0.3197, -0.8440],\n",
      "         [ 0.1240, -0.2647,  0.1389],\n",
      "         [-0.1040, -0.3369,  0.9580]],\n",
      "\n",
      "        [[-1.7125, -1.2589,  0.1095],\n",
      "         [-0.0731, -0.5805, -1.4300],\n",
      "         [ 0.3627,  1.1747,  2.5600],\n",
      "         [-0.1531, -0.6818, -1.6566]],\n",
      "\n",
      "        [[ 1.1510,  0.6730, -0.4737],\n",
      "         [-0.0960, -0.2659,  0.4583],\n",
      "         [-2.5486, -0.4815,  0.5106],\n",
      "         [ 0.3627,  1.1747,  2.5600]],\n",
      "\n",
      "        [[ 1.0877,  0.6921, -0.9737],\n",
      "         [-1.9039,  1.7196,  0.4717],\n",
      "         [-0.6503, -0.6275, -1.3836],\n",
      "         [-0.0960, -0.2659,  0.4583]],\n",
      "\n",
      "        [[-0.9901, -1.3036,  1.1135],\n",
      "         [ 0.2733,  0.4451,  0.7787],\n",
      "         [-0.1911,  0.2768,  0.1496],\n",
      "         [-0.7183, -0.1906, -0.6420]],\n",
      "\n",
      "        [[-0.0503,  1.5289,  1.3821],\n",
      "         [ 1.3004,  0.6449,  0.2163],\n",
      "         [-1.4673, -0.2747, -0.4649],\n",
      "         [ 0.3134,  0.9798, -1.7219]],\n",
      "\n",
      "        [[-1.4673, -0.2747, -0.4649],\n",
      "         [ 0.8160, -0.4037, -0.8977],\n",
      "         [ 0.2926,  0.4662, -0.7965],\n",
      "         [ 0.2867,  1.4247,  1.3072]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "# each token now has the assigned number of dimentions instead of being a single token ID\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scsHf8OgJiSX"
   },
   "source": [
    "### Create absolute positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "549fxfhkJh_V",
    "outputId": "9090e50d-5915-4b1b-93d2-1a4d34747ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0.2898,  1.3335, -0.0266],\n",
      "        [ 0.1003, -0.6332, -1.2612],\n",
      "        [ 0.2369, -0.6738,  1.5943],\n",
      "        [-0.1716,  0.6951,  0.1599]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# # [0, 1, 2, 3] \"column\" position is the position of each word on each sequence of 4 context_length\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# # uncomment & execute the following line to see how the embeddings look like\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3bdp26B93oU"
   },
   "source": [
    "### Create input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F00jaZjOKPpw",
    "outputId": "1d836257-87e3-43c9-f966-135e74523edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 3]) + torch.Size([4, 3]) = torch.Size([8, 4, 3])\n",
      "tensor([[[-1.2770,  0.9292,  1.9928],\n",
      "         [ 0.1321, -1.2209, -1.1229],\n",
      "         [ 0.3451, -0.3290,  2.1604],\n",
      "         [ 1.0359,  0.9019, -0.2860]],\n",
      "\n",
      "        [[ 0.6909,  1.7016, -0.5888],\n",
      "         [-1.3371, -0.9529, -2.1052],\n",
      "         [ 0.3609, -0.9385,  1.7333],\n",
      "         [-0.2756,  0.3582,  1.1179]],\n",
      "\n",
      "        [[-1.4228,  0.0746,  0.0829],\n",
      "         [ 0.0272, -1.2137, -2.6912],\n",
      "         [ 0.5995,  0.5009,  4.1544],\n",
      "         [-0.3246,  0.0133, -1.4967]],\n",
      "\n",
      "        [[ 1.4407,  2.0065, -0.5003],\n",
      "         [ 0.0043, -0.8991, -0.8029],\n",
      "         [-2.3118, -1.1553,  2.1049],\n",
      "         [ 0.1911,  1.8698,  2.7199]],\n",
      "\n",
      "        [[ 1.3775,  2.0256, -1.0003],\n",
      "         [-1.8036,  1.0864, -0.7895],\n",
      "         [-0.4134, -1.3013,  0.2108],\n",
      "         [-0.2675,  0.4292,  0.6182]],\n",
      "\n",
      "        [[-0.7004,  0.0298,  1.0869],\n",
      "         [ 0.3736, -0.1881, -0.4825],\n",
      "         [ 0.0458, -0.3970,  1.7439],\n",
      "         [-0.8899,  0.5044, -0.4821]],\n",
      "\n",
      "        [[ 0.2395,  2.8624,  1.3555],\n",
      "         [ 1.4007,  0.0117, -1.0449],\n",
      "         [-1.2304, -0.9485,  1.1294],\n",
      "         [ 0.1418,  1.6748, -1.5620]],\n",
      "\n",
      "        [[-1.1775,  1.0588, -0.4915],\n",
      "         [ 0.9163, -1.0369, -2.1589],\n",
      "         [ 0.5295, -0.2076,  0.7978],\n",
      "         [ 0.1151,  2.1198,  1.4671]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGLUALUt968u"
   },
   "source": [
    "### What about more dimmension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRFPsblZ-CfK"
   },
   "outputs": [],
   "source": [
    "# vocab_size = 50257\n",
    "# output_dim = 256 # 256 is a more common starting point\n",
    "\n",
    "# token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "# token_embeddings = token_embedding_layer(inputs)\n",
    "# print(token_embeddings.shape)\n",
    "# print(token_embeddings)\n",
    "\n",
    "# context_length = max_length\n",
    "# pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "# pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "# print(pos_embeddings.shape)\n",
    "# print(pos_embeddings)\n",
    "\n",
    "\n",
    "# input_embeddings = token_embeddings + pos_embeddings\n",
    "# print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
    "# print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbTcK0ieCyHO"
   },
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC3hdDBCQyxj"
   },
   "source": [
    "## Preparing data to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL4kUkq1FNVy"
   },
   "source": [
    "### Get input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUVjip9OAjSn"
   },
   "outputs": [],
   "source": [
    "def get_input_embeddings(my_raw_text):\n",
    "  dataloader = create_dataloader_v1(my_raw_text, batch_size=1, max_length=6, stride=6, shuffle=False)\n",
    "\n",
    "  data_iter = iter(dataloader)\n",
    "  # First and only batch\n",
    "  inputs, targets = next(data_iter)\n",
    "  # print(inputs)\n",
    "  # print(targets)  # ignore the targets\n",
    "\n",
    "  vocab_size = 50257\n",
    "  output_dim = 3\n",
    "\n",
    "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "  token_embeddings = token_embedding_layer(inputs)\n",
    "  context_length = 6\n",
    "  pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "  pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "  input_embeddings = token_embeddings + pos_embeddings\n",
    "  # print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
    "  # print(input_embeddings)\n",
    "  return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQzrouXuGICO",
    "outputId": "c12ea05f-c766-442a-8369-436a8424b13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2882, -1.0678, -1.1739],\n",
      "        [-0.6651, -3.8029,  0.4468],\n",
      "        [-1.1316,  1.3744, -0.2364],\n",
      "        [-1.0242,  0.8816,  1.2310],\n",
      "        [-2.4379, -2.3635, -1.9857],\n",
      "        [ 1.2236,  2.3880,  1.7501]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_raw_text = \"Your journey starts with one step\"\n",
    "small_input_embeddings = get_input_embeddings(my_raw_text)[0]\n",
    "print(small_input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkdLF-xTIFdB"
   },
   "outputs": [],
   "source": [
    "# tensor([\n",
    "#     [ 0.4113,  1.3397, -1.2234], Your    (x^1)\n",
    "#     [-1.8881, -0.0679, -1.1267], journey (x^2)\n",
    "#     [-0.2323, -2.2089, -1.6685], starts  (x^3)\n",
    "#     [ 0.5615,  1.2698,  2.5768], with    (x^4)\n",
    "#     [-0.9290, -0.0227,  0.6467], one     (x^5)\n",
    "#     [ 0.5691, -2.0627, -3.2411]  step    (x^6)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykfq4LulMWSG"
   },
   "source": [
    "### Forcing values to fit between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vf5_7By8Ma-V",
    "outputId": "37d259d9-0530-4127-d8a3-97dbbfcb915a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.1700],\n",
      "        [0.5800, 0.4600, 0.0300],\n",
      "        [0.4500, 0.4500, 0.0600],\n",
      "        [0.0100, 0.6600, 0.5700],\n",
      "        [0.4600, 0.6800, 0.4400],\n",
      "        [0.2600, 0.0700, 0.2200]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import decimal\n",
    "min_val = small_input_embeddings.min()\n",
    "max_val = small_input_embeddings.max()\n",
    "scaled_embeddings = (small_input_embeddings - min_val) / (max_val - min_val)\n",
    "rounded_embeddings = torch.round(scaled_embeddings * 100) / 100\n",
    "print(rounded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gYidQzDQAI5"
   },
   "source": [
    "## Simple self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UryRjq28FZ39"
   },
   "source": [
    "### Step 1 - Compute unormalized attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tc6wJC7RJU3c",
    "outputId": "1187d8b5-7b7e-4a45-f3fc-7d07a8c99c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of tensor([1.0000, 0.0000, 0.1700], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.4600, 0.0300], grad_fn=<SelectBackward0>)\n",
      "dot product of tensor([0.5800, 0.4600, 0.0300], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.4600, 0.0300], grad_fn=<SelectBackward0>)\n",
      "dot product of tensor([0.4500, 0.4500, 0.0600], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.4600, 0.0300], grad_fn=<SelectBackward0>)\n",
      "dot product of tensor([0.0100, 0.6600, 0.5700], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.4600, 0.0300], grad_fn=<SelectBackward0>)\n",
      "dot product of tensor([0.4600, 0.6800, 0.4400], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.4600, 0.0300], grad_fn=<SelectBackward0>)\n",
      "dot product of tensor([0.2600, 0.0700, 0.2200], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.4600, 0.0300], grad_fn=<SelectBackward0>)\n",
      "tensor([0.5851, 0.5489, 0.4698, 0.3265, 0.5928, 0.1896], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "query = rounded_embeddings[1]  # 2nd input token is the query)\n",
    "# just allocate a tensor in memory with 6 spaces\n",
    "attn_scores_2 = torch.empty(rounded_embeddings.shape[0])\n",
    "\n",
    "#fill the tensor with the dot products which multiply and sum\n",
    "for i, x_i in enumerate(rounded_embeddings):\n",
    "    print(f'dot product of {x_i} against journey {query}')\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMsw6j8WJsFU"
   },
   "source": [
    "### Step 2 - Normalize the attenton scores to sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PF1wtEJJrnj",
    "outputId": "b27c7e20-ac44-434f-c6b1-fa9f5f8fc30f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1884, 0.1817, 0.1679, 0.1454, 0.1898, 0.1268],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Sum: tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MB0l0Y-a090",
    "outputId": "09985572-e51b-4d14-d8af-132e746fd10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6, 4])\n",
      "4\n",
      "2\n",
      "3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Fooling around with dimensions\n",
    "my_tensor = torch.ones(2,3,6,4)\n",
    "# print(my_tensor)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor.shape[-1])\n",
    "print(my_tensor.shape[0])\n",
    "print(my_tensor.shape[1])\n",
    "print(my_tensor.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIYCzERyLwBQ",
    "outputId": "09036c32-44f2-4ba2-f0d8-eeb3db433ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1884, 0.1817, 0.1679, 0.1454, 0.1898, 0.1268],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Sum: tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# using pytorch softmax fucntion\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsNQPLyNQ9DN"
   },
   "source": [
    "### Step 3 - Compute the context vector $z^{(2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tba1xAyQ5G2",
    "outputId": "5a68dea0-61dd-4c18-a451-28bada5fd4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18836823105812073 * tensor([1.0000, 0.0000, 0.1700], grad_fn=<UnbindBackward0>)\n",
      "0.18167123198509216 * tensor([0.5800, 0.4600, 0.0300], grad_fn=<UnbindBackward0>)\n",
      "0.1678546965122223 * tensor([0.4500, 0.4500, 0.0600], grad_fn=<UnbindBackward0>)\n",
      "0.1454450935125351 * tensor([0.0100, 0.6600, 0.5700], grad_fn=<UnbindBackward0>)\n",
      "0.18982425332069397 * tensor([0.4600, 0.6800, 0.4400], grad_fn=<UnbindBackward0>)\n",
      "0.12683647871017456 * tensor([0.2600, 0.0700, 0.2200], grad_fn=<UnbindBackward0>)\n",
      "tensor(1.0000, grad_fn=<AddBackward0>)\n",
      "tensor([0.4910, 0.3931, 0.2419], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "query = rounded_embeddings[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "attn_weights_sum = 0\n",
    "for i,x_i in enumerate(rounded_embeddings):\n",
    "    print(f'{attn_weights_2[i]} * {x_i}')\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    attn_weights_sum += attn_weights_2[i]\n",
    "\n",
    "print(attn_weights_sum)\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtDPseV2Tgyp"
   },
   "source": [
    "### Get All attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwDMWyFwTgmA",
    "outputId": "bfab7a25-b7fe-4b7b-c6c7-afef22290e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0289, 0.5851, 0.4602, 0.1069, 0.5348, 0.2974],\n",
      "        [0.5851, 0.5489, 0.4698, 0.3265, 0.5928, 0.1896],\n",
      "        [0.4602, 0.4698, 0.4086, 0.3357, 0.5394, 0.1617],\n",
      "        [0.1069, 0.3265, 0.3357, 0.7606, 0.7042, 0.1742],\n",
      "        [0.5348, 0.5928, 0.5394, 0.7042, 0.8676, 0.2640],\n",
      "        [0.2974, 0.1896, 0.1617, 0.1742, 0.2640, 0.1209]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(rounded_embeddings):\n",
    "    for j, x_j in enumerate(rounded_embeddings):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvNIm4n3TsBT"
   },
   "source": [
    "We can achive the same but more efficiently via matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaHP2VdZCouE",
    "outputId": "918870df-34b6-485b-a68f-b23fdab2e699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([3, 6])\n",
      "-------------------\n",
      "tensor([[1.0000, 0.0000, 0.1700],\n",
      "        [0.5800, 0.4600, 0.0300],\n",
      "        [0.4500, 0.4500, 0.0600],\n",
      "        [0.0100, 0.6600, 0.5700],\n",
      "        [0.4600, 0.6800, 0.4400],\n",
      "        [0.2600, 0.0700, 0.2200]], grad_fn=<DivBackward0>)\n",
      "-------------------\n",
      "tensor([[1.0000, 0.5800, 0.4500, 0.0100, 0.4600, 0.2600],\n",
      "        [0.0000, 0.4600, 0.4500, 0.6600, 0.6800, 0.0700],\n",
      "        [0.1700, 0.0300, 0.0600, 0.5700, 0.4400, 0.2200]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using matrix transpose (remember is row * column)\n",
    "print(rounded_embeddings.shape)\n",
    "print(rounded_embeddings.T.shape)\n",
    "print('-------------------')\n",
    "print(rounded_embeddings)\n",
    "print('-------------------')\n",
    "print(rounded_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MeXFpyjTwRW",
    "outputId": "709d4c46-ca53-41c3-fec0-9bfb2a7c4fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0289, 0.5851, 0.4602, 0.1069, 0.5348, 0.2974],\n",
      "        [0.5851, 0.5489, 0.4698, 0.3265, 0.5928, 0.1896],\n",
      "        [0.4602, 0.4698, 0.4086, 0.3357, 0.5394, 0.1617],\n",
      "        [0.1069, 0.3265, 0.3357, 0.7606, 0.7042, 0.1742],\n",
      "        [0.5348, 0.5928, 0.5394, 0.7042, 0.8676, 0.2640],\n",
      "        [0.2974, 0.1896, 0.1617, 0.1742, 0.2640, 0.1209]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = rounded_embeddings @ rounded_embeddings.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbaMTFAmT4jD"
   },
   "source": [
    "Apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNxM31OqT3r2",
    "outputId": "45a20d00-68ce-44c1-a3c0-3e820847365d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2705, 0.1735, 0.1532, 0.1076, 0.1650, 0.1302],\n",
      "        [0.1884, 0.1817, 0.1679, 0.1454, 0.1898, 0.1268],\n",
      "        [0.1765, 0.1782, 0.1676, 0.1558, 0.1910, 0.1309],\n",
      "        [0.1203, 0.1499, 0.1512, 0.2313, 0.2186, 0.1287],\n",
      "        [0.1561, 0.1654, 0.1568, 0.1849, 0.2177, 0.1191],\n",
      "        [0.1831, 0.1644, 0.1599, 0.1619, 0.1771, 0.1535]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dim=-1 is to apply softmax to the last dimenison, in this case rows\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPREekd0T-_j",
    "outputId": "7187a737-d431-4e0d-c579-a5077209997b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "row_0_sum = sum([0.1403, 0.1365, 0.1915, 0.1552, 0.1659, 0.2106])\n",
    "print(row_0_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y40kr-uUM19"
   },
   "source": [
    "### Compute All context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATCLH64zUMp-",
    "outputId": "221da437-408a-4430-d846-39669bf6b7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5509, 0.3411, 0.2230],\n",
      "        [0.4910, 0.3931, 0.2419],\n",
      "        [0.4787, 0.3993, 0.2471],\n",
      "        [0.4116, 0.4473, 0.2904],\n",
      "        [0.4556, 0.4251, 0.2683],\n",
      "        [0.4735, 0.3856, 0.2497]], grad_fn=<MmBackward0>)\n",
      "Previous 2nd context vector: tensor([0.4910, 0.3931, 0.2419], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ rounded_embeddings\n",
    "print(all_context_vecs)\n",
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VghIaCejQ6JH"
   },
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1218PmTRnuh"
   },
   "source": [
    "**Weight parameters** are learned coefficients that define the network connections, while **attention weights** are dynamic, context-specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObbzoVnCShzc",
    "outputId": "c313eb75-f955-4966-92fb-b9c8c5ce16e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "x_2 = rounded_embeddings[1] # second input element\n",
    "d_in = rounded_embeddings.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "print(W_query)\n",
    "print(W_key)\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TC8hnCwUce2",
    "outputId": "d8826549-56c3-4e1b-cad0-ba43f4142978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2897, 0.6423], grad_fn=<SqueezeBackward4>)\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)\n",
    "\n",
    "# calculate keys and values vector for all inputs\n",
    "keys = rounded_embeddings @ W_key\n",
    "values = rounded_embeddings @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xi-rk9d_U3_8",
    "outputId": "397903df-18bd-45d2-f12a-1902860d735d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3163, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2w0_CCgU030",
    "outputId": "9acab22a-9926-4678-d881-c48ab10b7d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1960, 0.3163, 0.3134, 0.6479, 0.6364, 0.1810],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgtjdYYBVrjm"
   },
   "source": [
    "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension,  ùëëùëò‚Äæ‚Äæ‚Äæ‚àö  (i.e., d_k**0.5):\n",
    "\n",
    "Imagine you have two vectors, and their dot product results in a large value. When this large value is passed through the softmax function, it might dominate the probabilities, making the attention mechanism less sensitive to other relevant parts of the input. Scaling helps to mitigate this issue by preventing any single dot product from becoming overly influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFekEwTuVWFt",
    "outputId": "7742d4b9-51c0-4cca-c49d-231f6df02297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1448, 0.1576, 0.1573, 0.1993, 0.1977, 0.1433],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1] # dimension of keys\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzEw5VL0XQqE",
    "outputId": "2262c0bb-fbc5-4005-b7cf-2c79e3940771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1979, 0.4785], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-Dg25JCXccF"
   },
   "source": [
    "### Compact SelfAttention Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPFiRyaKX0Lb"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZzWub2DXvao"
   },
   "source": [
    "- We can streamline the implementation above using PyTorch's Linear layers, which are equivalent to a matrix multiplication if we disable the bias units\n",
    "- Another big advantage of using `nn.Linear` over our manual `nn.Parameter(torch.rand(...)` approach is that `nn.Linear` has a preferred weight initialization scheme, which leads to more stable model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yx74nUgLfNJB"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O5vlV-nXw3p"
   },
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2mJv-ETX-g3",
    "outputId": "10c429ed-906f-40fc-c106-811061cf3d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "tensor([[0.0160, 0.1600],\n",
      "        [0.0148, 0.1583],\n",
      "        [0.0147, 0.1582],\n",
      "        [0.0175, 0.1624],\n",
      "        [0.0176, 0.1627],\n",
      "        [0.0148, 0.1582]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(rounded_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLpyQ2keYaxo"
   },
   "source": [
    "### Hiding futer words with causal attention (one step back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd9vAnBTYjJp",
    "outputId": "8ad2d760-e3db-405b-e55d-a553faf1e5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1797, 0.1623, 0.1612, 0.1626, 0.1659, 0.1683],\n",
      "        [0.1776, 0.1613, 0.1606, 0.1652, 0.1674, 0.1679],\n",
      "        [0.1769, 0.1617, 0.1611, 0.1652, 0.1672, 0.1678],\n",
      "        [0.1860, 0.1601, 0.1586, 0.1608, 0.1656, 0.1690],\n",
      "        [0.1878, 0.1587, 0.1572, 0.1612, 0.1661, 0.1691],\n",
      "        [0.1744, 0.1644, 0.1638, 0.1638, 0.1660, 0.1677]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse data from previous section\n",
    "queries = sa_v2.W_query(rounded_embeddings)\n",
    "keys = sa_v2.W_key(rounded_embeddings)\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKX_phRVdlvv"
   },
   "source": [
    "Applying negative infinity effectively zeros out the probabilities for these future tokens in the subsequent softmax calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tmtxjas9cwln",
    "outputId": "1b17a71d-6906-4c0a-96ac-4250485330b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1565,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1500, 0.0135,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1393, 0.0125, 0.0064,   -inf,   -inf,   -inf],\n",
      "        [0.2310, 0.0187, 0.0054, 0.0253,   -inf,   -inf],\n",
      "        [0.2599, 0.0217, 0.0078, 0.0433, 0.0857,   -inf],\n",
      "        [0.0901, 0.0069, 0.0012, 0.0015, 0.0200, 0.0348]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[-1]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5mLgJvOdGAr",
    "outputId": "cb6d30b6-81cd-448c-d478-6dfff070f682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5241, 0.4759, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3540, 0.3237, 0.3223, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2795, 0.2405, 0.2383, 0.2417, 0.0000, 0.0000],\n",
      "        [0.2261, 0.1910, 0.1891, 0.1939, 0.1998, 0.0000],\n",
      "        [0.1744, 0.1644, 0.1638, 0.1638, 0.1660, 0.1677]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-XqXxKcYjbe"
   },
   "source": [
    "### Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGyeNovHe97q",
    "outputId": "74f12cec-8e3b-4968-8572-06c814426c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6446, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4811, 0.0000, 0.4833, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3820, 0.3783, 0.3879, 0.3997, 0.0000],\n",
      "        [0.3488, 0.3288, 0.0000, 0.0000, 0.3319, 0.3354]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
    "example = torch.ones(6, 6) # create a matrix of ones\n",
    "\n",
    "print(dropout(example))\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9schnK0fRte"
   },
   "source": [
    "### Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8GxHW4IfRVU"
   },
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        print(b)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eoq6pVwfyzu",
    "outputId": "b2927501-6272-4f48-aeb1-fc4d541c93ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[-0.5261, -0.0492],\n",
      "         [-0.4886, -0.1538],\n",
      "         [-0.4559, -0.1782],\n",
      "         [-0.4310, -0.1523],\n",
      "         [-0.4569, -0.1589],\n",
      "         [-0.4150, -0.1289]],\n",
      "\n",
      "        [[-0.5261, -0.0492],\n",
      "         [-0.4886, -0.1538],\n",
      "         [-0.4559, -0.1782],\n",
      "         [-0.4310, -0.1523],\n",
      "         [-0.4569, -0.1589],\n",
      "         [-0.4150, -0.1289]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.stack((rounded_embeddings, rounded_embeddings), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UydLlF4rj0hf"
   },
   "source": [
    "## Self Attention multi-head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0f3nXA9ne-Q"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAZxS6r3nWzN",
    "outputId": "bcf0d4d0-9244-43fe-b8d3-5239af1d7621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "torch.Size([2, 6, 2, 1]) vs torch.Size([2, 2, 6, 1])\n",
      "--------------------\n",
      "tensor([[[0.2699, 0.4130],\n",
      "         [0.2399, 0.4178],\n",
      "         [0.2289, 0.4308],\n",
      "         [0.2306, 0.4467],\n",
      "         [0.2335, 0.4329],\n",
      "         [0.2335, 0.4581]],\n",
      "\n",
      "        [[0.2699, 0.4130],\n",
      "         [0.2399, 0.4178],\n",
      "         [0.2289, 0.4308],\n",
      "         [0.2306, 0.4467],\n",
      "         [0.2335, 0.4329],\n",
      "         [0.2335, 0.4581]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #b is for batches\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        print('--------------------')\n",
    "        print(f'{keys.shape} vs {keys.transpose(1,2).shape}')\n",
    "        print('--------------------')\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "123g1w8Vz_0T",
    "KiqznPKxE2-J",
    "kQyY8us8kjnU",
    "sdqVKwHw9WOr",
    "e5oaJW523vt2",
    "b0nYzc5uBMkF",
    "cXjmjDeM6z-5",
    "t-qc5coF7P9c",
    "scsHf8OgJiSX",
    "N3bdp26B93oU",
    "JGLUALUt968u",
    "SC3hdDBCQyxj",
    "4gYidQzDQAI5",
    "UryRjq28FZ39"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
