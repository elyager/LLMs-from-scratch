{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elyager/LLMs-from-scratch/blob/main/LLM_from_scratch_personal_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123g1w8Vz_0T"
      },
      "source": [
        "### Requiriments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCSj5qDn9bP-",
        "outputId": "0e55ced8-896e-4e2e-92bd-8fa09a249fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqznPKxE2-J"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQd3ILu5Czb"
      },
      "source": [
        "- **vocab_size:** es el tamaño del vocabulario en tokens únicos disponibles+extensiones. En nuestro caso dado por el tokenizador creado con BPE.\n",
        "- **output_dim:** el número de dimensiones de cada token. Las dimensiones describen a una palabra o concepto. Más dimensiones capturan más detalles.\n",
        "- **max_length:** es la máxima logitud de tokens por secuencia.\n",
        "- **batch_size:** es cuantas secuencias (muestras de texto) tiene cada batch. Larger batch sizes can speed up training but might require more memory.\n",
        "- **stride:** el tamaño de la zancada en tokens, cuantos tokens salta para la siguiente secuencia, esto determina que tanto se empalma una secuencia con otra. Ensure the model sees the context of each token multiple times during training, helping it learn better relationships between words.\n",
        "- **shuffle:** determina si le da un orden aleatorio a las secuencias para que sea random en cada epoc. Prevents the model from learning patterns based on the order of data presentation. It helps generalize learning and avoid overfitting to a specific data order.\n",
        "-**drop_last:** indica si se debe descartar el último batch cuando no cumple con el número de muestras establecido en max_lenght. Si se tiene un set de datos pequeño es mejor no hacer drop.\n",
        "-**num_workers:** es el número de procesos, a mayor número más rápidez.\n",
        "-**attention score:** determina qué tan \"similar\" es una pababra con la otra a través de dot product que es un tipo de similarity function. Mayor atention score mayor similitud entre los números.\n",
        "--**attention weight:** es la versión normalizada a través de softmax de los attention scores.\n",
        "-**context vector:** es un embedding vector pero que tiene todo el contexto del resto de input vectors. Se obtiene sumando todos los attention weights del inpute secuence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZbR6dXofBXJx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN__3TwCBXiO"
      },
      "source": [
        "# Chapter 1 & 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyY8us8kjnU",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load the text for training (our corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLL4xfSX9C0p",
        "outputId": "a8ff33b3-19bf-4905-888d-faa5cb181c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqVKwHw9WOr",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### BytePair Encoding (BPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmSakIi9grv",
        "outputId": "f5f3c34b-1d4f-4178-8a99-52375748595e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoAAtvXt9r53",
        "outputId": "3431c86e-59fd-453e-8511-b5a8d019da82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I |-| H |-|AD |-| always |-| thought |-| Jack |-| G |-|is |-|burn |-| rather\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n",
            "\n",
            " Total of tokens: 5146\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
        "enc_text.append(tokenizer.eot_token)\n",
        "\n",
        "# First 10 tokens from raw_text\n",
        "first_10_token_ids = enc_text[:10]\n",
        "decoded_tokens = [tokenizer.decode([token_id]) for token_id in first_10_token_ids]\n",
        "delimited_tokens = ' |-|'.join(decoded_tokens)\n",
        "print(delimited_tokens)\n",
        "print(enc_text[:10])\n",
        "print(f'\\n Total of tokens: {len(enc_text)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5oaJW523vt2",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Dataset loader (creating tokenIDs for inputs and targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "o6B2KxAkAbLC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        token_ids.append(tokenizer.eot_token)\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# dataset = GPTDatasetV1(raw_text, tokenizer, max_length=4, stride=1)\n",
        "# print(dataset.input_ids)\n",
        "# print(dataset.target_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KXlE1wG6AsOk"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0nYzc5uBMkF"
      },
      "source": [
        "### Use DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gIJnkOxAvfj",
        "outputId": "c302f572-19d6-41eb-d910-af5a9e6a7ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "# for batch in dataloader:\n",
        "#     input, target = batch\n",
        "#     print(input, target)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch) # input and target\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch) # input and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Oyu9uJEay2",
        "outputId": "445e55b4-5d47-44d2-9302-907a4ce66c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "Inputs:\n",
            " I HAD always\n",
            "\n",
            "Targets:\n",
            "  HAD always thought\n",
            "Inputs:\n",
            " tensor([[  287,   262,  6001,   286],\n",
            "        [  465, 13476,    11,   339],\n",
            "        [  550,  5710,   465, 12036],\n",
            "        [   11,  6405,   257,  5527],\n",
            "        [27075,    11,   290,  4920],\n",
            "        [ 2241,   287,   257,  4489],\n",
            "        [   64,   319,   262, 34686],\n",
            "        [41976,    13,   357, 10915]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  262,  6001,   286,   465],\n",
            "        [13476,    11,   339,   550],\n",
            "        [ 5710,   465, 12036,    11],\n",
            "        [ 6405,   257,  5527, 27075],\n",
            "        [   11,   290,  4920,  2241],\n",
            "        [  287,   257,  4489,    64],\n",
            "        [  319,   262, 34686, 41976],\n",
            "        [   13,   357, 10915,   314]])\n",
            "Inputs:\n",
            "  in the height of\n",
            "\n",
            "Targets:\n",
            "  the height of his\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "# First batch\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
        "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))\n",
        "\n",
        "# Second batch\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
        "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXjmjDeM6z-5"
      },
      "source": [
        "### Create our token embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax4UNR13E4wB",
        "outputId": "735845ba-7241-4d94-df3d-9af21c31b832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257, 3])\n",
            "Parameter containing:\n",
            "tensor([[ 0.2253,  0.0226,  0.9778],\n",
            "        [-0.5548,  1.7454, -0.3383],\n",
            "        [-0.1281,  0.7334, -0.0365],\n",
            "        ...,\n",
            "        [ 1.4080,  0.5984,  1.2340],\n",
            "        [ 1.0614,  0.6226, -2.2400],\n",
            "        [ 0.9814,  0.5242,  0.3422]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 3 # 256 is a more common starting point\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(token_embedding_layer.weight.shape)\n",
        "print(token_embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaPfzy6d631b"
      },
      "source": [
        "#### Load our dataset to get the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuqLpfanGGBp",
        "outputId": "96b89392-1b10-4f95-e390-2129a4d521e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Token IDs:\\n\",  inputs) # we take the first batch and  ignore the targets for now\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-qc5coF7P9c"
      },
      "source": [
        "### Create token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ziD4fWmIUMy",
        "outputId": "bb89af98-6b5f-4f84-f976-7bf7244fcd1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 3])\n",
            "tensor([[[ 0.2603, -0.8816,  0.1043],\n",
            "         [ 0.2764, -0.4621, -0.7715],\n",
            "         [ 0.8915, -1.0904, -0.2301],\n",
            "         [-0.6681, -0.8020,  0.3033]],\n",
            "\n",
            "        [[ 0.8063, -0.0582, -1.7538],\n",
            "         [-0.3572,  0.4484,  1.3695],\n",
            "         [-0.7448,  0.2903,  0.0090],\n",
            "         [-1.9627, -0.5759,  2.8155]],\n",
            "\n",
            "        [[ 1.5792, -0.2801,  0.4912],\n",
            "         [ 0.6434,  0.0310,  0.0791],\n",
            "         [-0.4761, -1.8900,  0.4606],\n",
            "         [ 0.4095,  0.7004,  0.0515]],\n",
            "\n",
            "        [[-1.2447, -0.6668,  0.5984],\n",
            "         [ 0.4962, -1.0619,  1.0492],\n",
            "         [-0.4329,  0.0821, -0.4267],\n",
            "         [-0.4761, -1.8900,  0.4606]],\n",
            "\n",
            "        [[-1.6483, -1.1164,  1.1943],\n",
            "         [ 1.3296, -1.7755,  1.0300],\n",
            "         [-1.0391, -0.5687, -1.3244],\n",
            "         [ 0.4962, -1.0619,  1.0492]],\n",
            "\n",
            "        [[ 0.8674,  0.3860,  1.9747],\n",
            "         [-0.1526,  0.5415, -1.1297],\n",
            "         [ 2.2568,  1.4764,  1.6068],\n",
            "         [-0.2412,  0.1579, -1.8213]],\n",
            "\n",
            "        [[-1.0868,  0.0871, -0.7187],\n",
            "         [-1.0221,  0.2430,  0.2440],\n",
            "         [ 0.7314, -0.9323, -0.4208],\n",
            "         [-1.9216,  0.5946,  2.0861]],\n",
            "\n",
            "        [[ 0.7314, -0.9323, -0.4208],\n",
            "         [-0.5868,  0.1165, -0.2218],\n",
            "         [ 1.2842,  1.3900, -1.2463],\n",
            "         [-0.8436,  1.7795, -0.5409]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "# each token now has the assigned number of dimentions instead of being a single token ID\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(token_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scsHf8OgJiSX"
      },
      "source": [
        "### Create absolute positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "549fxfhkJh_V",
        "outputId": "6639a8e4-a6d1-406e-84b7-79e6fa114ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n",
            "tensor([[ 0.1183, -1.5899, -1.0680],\n",
            "        [-1.4079, -2.6071, -0.4584],\n",
            "        [-1.4400,  0.6619, -1.3333],\n",
            "        [ 0.5600, -0.0192, -0.0558]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "\n",
        "# # [0, 1, 2, 3] \"column\" position is the position of each word on each sequence of 4 context_length\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "# # uncomment & execute the following line to see how the embeddings look like\n",
        "print(pos_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3bdp26B93oU"
      },
      "source": [
        "### Create input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F00jaZjOKPpw",
        "outputId": "55bcc64a-dd30-43f3-ed19-78376c818793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 3]) + torch.Size([4, 3]) = torch.Size([8, 4, 3])\n",
            "tensor([[[ 3.7859e-01, -2.4715e+00, -9.6366e-01],\n",
            "         [-1.1315e+00, -3.0692e+00, -1.2299e+00],\n",
            "         [-5.4848e-01, -4.2846e-01, -1.5634e+00],\n",
            "         [-1.0804e-01, -8.2112e-01,  2.4746e-01]],\n",
            "\n",
            "        [[ 9.2461e-01, -1.6481e+00, -2.8217e+00],\n",
            "         [-1.7651e+00, -2.1587e+00,  9.1108e-01],\n",
            "         [-2.1848e+00,  9.5214e-01, -1.3243e+00],\n",
            "         [-1.4027e+00, -5.9503e-01,  2.7597e+00]],\n",
            "\n",
            "        [[ 1.6975e+00, -1.8701e+00, -5.7673e-01],\n",
            "         [-7.6454e-01, -2.5761e+00, -3.7935e-01],\n",
            "         [-1.9160e+00, -1.2282e+00, -8.7274e-01],\n",
            "         [ 9.6952e-01,  6.8121e-01, -4.2917e-03]],\n",
            "\n",
            "        [[-1.1264e+00, -2.2568e+00, -4.6952e-01],\n",
            "         [-9.1170e-01, -3.6690e+00,  5.9074e-01],\n",
            "         [-1.8728e+00,  7.4394e-01, -1.7600e+00],\n",
            "         [ 8.3957e-02, -1.9092e+00,  4.0476e-01]],\n",
            "\n",
            "        [[-1.5301e+00, -2.7064e+00,  1.2634e-01],\n",
            "         [-7.8336e-02, -4.3826e+00,  5.7160e-01],\n",
            "         [-2.4791e+00,  9.3223e-02, -2.6578e+00],\n",
            "         [ 1.0563e+00, -1.0811e+00,  9.9334e-01]],\n",
            "\n",
            "        [[ 9.8565e-01, -1.2039e+00,  9.0678e-01],\n",
            "         [-1.5605e+00, -2.0656e+00, -1.5881e+00],\n",
            "         [ 8.1684e-01,  2.1383e+00,  2.7350e-01],\n",
            "         [ 3.1884e-01,  1.3870e-01, -1.8771e+00]],\n",
            "\n",
            "        [[-9.6852e-01, -1.5029e+00, -1.7866e+00],\n",
            "         [-2.4301e+00, -2.3641e+00, -2.1444e-01],\n",
            "         [-7.0862e-01, -2.7043e-01, -1.7541e+00],\n",
            "         [-1.3616e+00,  5.7546e-01,  2.0303e+00]],\n",
            "\n",
            "        [[ 8.4963e-01, -2.5223e+00, -1.4887e+00],\n",
            "         [-1.9947e+00, -2.4906e+00, -6.8023e-01],\n",
            "         [-1.5580e-01,  2.0519e+00, -2.5796e+00],\n",
            "         [-2.8356e-01,  1.7604e+00, -5.9676e-01]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(input_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGLUALUt968u"
      },
      "source": [
        "### What about more dimmension?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SRFPsblZ-CfK",
        "outputId": "6b7dfdca-d974-449f-ba79-c6ca9f11b3fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "tensor([[[ 1.6362, -0.9072,  1.1920,  ..., -2.6416, -0.6914, -0.3487],\n",
            "         [-1.7458,  1.2970,  1.5771,  ...,  0.1099, -0.0311, -1.3486],\n",
            "         [-0.9854, -0.8970, -0.7286,  ...,  0.8005, -0.0573,  2.1166],\n",
            "         [ 0.0336,  2.2179,  1.0534,  ..., -0.4926,  1.3052,  1.7788]],\n",
            "\n",
            "        [[-0.8723, -0.9800, -2.3819,  ..., -0.1328,  0.1684,  0.4709],\n",
            "         [ 0.5318,  0.9099, -1.6467,  ...,  1.4716,  2.3577, -0.5886],\n",
            "         [ 1.2070,  0.7688,  0.2764,  ..., -0.8464, -0.4858, -1.4624],\n",
            "         [ 0.3716,  0.9414,  0.9643,  ..., -0.7349, -0.9102,  0.5917]],\n",
            "\n",
            "        [[ 1.8858, -0.5925, -0.4191,  ...,  0.5168, -1.0823, -0.3816],\n",
            "         [-0.8509,  0.7499, -0.9045,  ..., -1.6122,  0.3141,  1.6365],\n",
            "         [-2.7887, -0.1300,  1.0576,  ...,  0.8582, -0.2795, -0.0770],\n",
            "         [-0.8591,  0.8062,  0.2073,  ...,  1.6089, -0.0676, -1.3474]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.6342,  0.2042, -1.4248,  ...,  0.5370,  0.4192, -1.5422],\n",
            "         [-0.0173,  0.8601,  1.4499,  ...,  0.6973,  0.3269,  2.5880],\n",
            "         [ 0.7299,  0.7074,  0.4571,  ...,  0.8786,  0.5231, -0.0252],\n",
            "         [-0.1700,  0.9892,  1.5628,  ...,  1.7612,  0.3798, -0.2115]],\n",
            "\n",
            "        [[ 0.7485,  0.5041, -0.8995,  ..., -0.7751,  0.6215, -0.1089],\n",
            "         [-0.3099,  0.6840,  0.0940,  ..., -0.9189,  0.6957,  0.9603],\n",
            "         [-0.5328, -1.2167, -0.1099,  ...,  0.6798,  1.0584, -1.4496],\n",
            "         [-0.8584, -1.2336,  0.1396,  ...,  0.1411,  1.0419, -2.1736]],\n",
            "\n",
            "        [[-0.5328, -1.2167, -0.1099,  ...,  0.6798,  1.0584, -1.4496],\n",
            "         [-1.1009,  2.0259, -0.4785,  ...,  0.3195, -0.7439,  0.5563],\n",
            "         [ 0.2077,  0.1698, -0.6159,  ..., -0.0554, -0.4831,  1.7669],\n",
            "         [-0.6470,  0.3107,  1.4658,  ..., -1.2485,  1.3782, -0.1801]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([4, 256])\n",
            "tensor([[-0.6110,  0.5190,  0.7786,  ..., -1.0952, -0.0830, -0.3765],\n",
            "        [ 0.2768, -1.0714, -0.2321,  ..., -0.5560, -1.6807, -0.6236],\n",
            "        [-0.1269, -2.4689, -1.5185,  ...,  0.2663,  0.1347,  0.9554],\n",
            "        [ 0.0161,  0.8251, -0.2078,  ..., -0.4536, -1.6445, -1.8959]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([8, 4, 256]) + torch.Size([4, 256]) = torch.Size([8, 4, 256])\n",
            "tensor([[[ 1.0252e+00, -3.8821e-01,  1.9706e+00,  ..., -3.7368e+00,\n",
            "          -7.7439e-01, -7.2513e-01],\n",
            "         [-1.4690e+00,  2.2554e-01,  1.3450e+00,  ..., -4.4607e-01,\n",
            "          -1.7117e+00, -1.9722e+00],\n",
            "         [-1.1124e+00, -3.3660e+00, -2.2471e+00,  ...,  1.0668e+00,\n",
            "           7.7413e-02,  3.0720e+00],\n",
            "         [ 4.9690e-02,  3.0430e+00,  8.4561e-01,  ..., -9.4613e-01,\n",
            "          -3.3930e-01, -1.1710e-01]],\n",
            "\n",
            "        [[-1.4833e+00, -4.6094e-01, -1.6033e+00,  ..., -1.2280e+00,\n",
            "           8.5394e-02,  9.4479e-02],\n",
            "         [ 8.0861e-01, -1.6156e-01, -1.8788e+00,  ...,  9.1558e-01,\n",
            "           6.7702e-01, -1.2123e+00],\n",
            "         [ 1.0800e+00, -1.7001e+00, -1.2421e+00,  ..., -5.8006e-01,\n",
            "          -3.5109e-01, -5.0708e-01],\n",
            "         [ 3.8769e-01,  1.7665e+00,  7.5653e-01,  ..., -1.1885e+00,\n",
            "          -2.5547e+00, -1.3043e+00]],\n",
            "\n",
            "        [[ 1.2748e+00, -7.3487e-02,  3.5946e-01,  ..., -5.7838e-01,\n",
            "          -1.1652e+00, -7.5807e-01],\n",
            "         [-5.7409e-01, -3.2149e-01, -1.1366e+00,  ..., -2.1682e+00,\n",
            "          -1.3665e+00,  1.0129e+00],\n",
            "         [-2.9157e+00, -2.5990e+00, -4.6089e-01,  ...,  1.1245e+00,\n",
            "          -1.4483e-01,  8.7831e-01],\n",
            "         [-8.4303e-01,  1.6314e+00, -4.9244e-04,  ...,  1.1553e+00,\n",
            "          -1.7121e+00, -3.2433e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.2452e+00,  7.2319e-01, -6.4621e-01,  ..., -5.5813e-01,\n",
            "           3.3622e-01, -1.9186e+00],\n",
            "         [ 2.5948e-01, -2.1132e-01,  1.2177e+00,  ...,  1.4135e-01,\n",
            "          -1.3537e+00,  1.9644e+00],\n",
            "         [ 6.0299e-01, -1.7616e+00, -1.0614e+00,  ...,  1.1449e+00,\n",
            "           6.5780e-01,  9.3020e-01],\n",
            "         [-1.5392e-01,  1.8143e+00,  1.3550e+00,  ...,  1.3076e+00,\n",
            "          -1.2647e+00, -2.1074e+00]],\n",
            "\n",
            "        [[ 1.3748e-01,  1.0232e+00, -1.2089e-01,  ..., -1.8702e+00,\n",
            "           5.3856e-01, -4.8534e-01],\n",
            "         [-3.3114e-02, -3.8742e-01, -1.3812e-01,  ..., -1.4749e+00,\n",
            "          -9.8499e-01,  3.3669e-01],\n",
            "         [-6.5976e-01, -3.6857e+00, -1.6284e+00,  ...,  9.4613e-01,\n",
            "           1.1931e+00, -4.9428e-01],\n",
            "         [-8.4232e-01, -4.0850e-01, -6.8144e-02,  ..., -3.1251e-01,\n",
            "          -6.0261e-01, -4.0696e+00]],\n",
            "\n",
            "        [[-1.1438e+00, -6.9772e-01,  6.6869e-01,  ..., -4.1533e-01,\n",
            "           9.7543e-01, -1.8261e+00],\n",
            "         [-8.2415e-01,  9.5445e-01, -7.1062e-01,  ..., -2.3645e-01,\n",
            "          -2.4246e+00, -6.7278e-02],\n",
            "         [ 8.0799e-02, -2.2992e+00, -2.1343e+00,  ...,  2.1086e-01,\n",
            "          -3.4847e-01,  2.7223e+00],\n",
            "         [-6.3090e-01,  1.1358e+00,  1.2580e+00,  ..., -1.7021e+00,\n",
            "          -2.6628e-01, -2.0760e+00]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def more_more_more_dimensions(dim):\n",
        "  vocab_size = 50257\n",
        "  output_dim = dim\n",
        "\n",
        "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "  token_embeddings = token_embedding_layer(inputs)\n",
        "  print(token_embeddings.shape)\n",
        "  print(token_embeddings)\n",
        "\n",
        "  pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "  pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "  print(pos_embeddings.shape)\n",
        "  print(pos_embeddings)\n",
        "\n",
        "\n",
        "  input_embeddings = token_embeddings + pos_embeddings\n",
        "  print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "  print(input_embeddings)\n",
        "\n",
        "more_more_more_dimensions(256)  #256 is a more common starting point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTcK0ieCyHO"
      },
      "source": [
        "# Chapter 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC3hdDBCQyxj"
      },
      "source": [
        "## Preparing data to work with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL4kUkq1FNVy"
      },
      "source": [
        "### Get input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DUVjip9OAjSn"
      },
      "outputs": [],
      "source": [
        "def get_input_embeddings(my_raw_text):\n",
        "  dataloader = create_dataloader_v1(my_raw_text, batch_size=1, max_length=6, stride=6, shuffle=False)\n",
        "\n",
        "  data_iter = iter(dataloader)\n",
        "  # First and only batch\n",
        "  inputs, targets = next(data_iter)\n",
        "  # print(inputs)\n",
        "  # print(targets)  # ignore the targets\n",
        "\n",
        "  vocab_size = 50257\n",
        "  output_dim = 3\n",
        "\n",
        "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "  token_embeddings = token_embedding_layer(inputs)\n",
        "  context_length = 6\n",
        "  pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "  pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "  input_embeddings = token_embeddings + pos_embeddings\n",
        "  # print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "  # print(input_embeddings)\n",
        "  return input_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQzrouXuGICO",
        "outputId": "2a7c2049-55d5-48a1-c319-c4e5d8735e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.2861, -0.6132, -1.6731],\n",
            "        [-0.4499, -2.7230,  0.6959],\n",
            "        [-0.8747, -1.1428, -2.6916],\n",
            "        [ 0.3154,  0.2821, -1.1845],\n",
            "        [ 0.7820,  0.5111, -1.1167],\n",
            "        [-1.3217,  1.8591,  2.0771]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "my_raw_text = \"Your journey starts with one step\"\n",
        "small_input_embeddings = get_input_embeddings(my_raw_text)[0]\n",
        "print(small_input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkdLF-xTIFdB"
      },
      "outputs": [],
      "source": [
        "# tensor([\n",
        "#     [ 0.4113,  1.3397, -1.2234], Your    (x^1)\n",
        "#     [-1.8881, -0.0679, -1.1267], journey (x^2)\n",
        "#     [-0.2323, -2.2089, -1.6685], starts  (x^3)\n",
        "#     [ 0.5615,  1.2698,  2.5768], with    (x^4)\n",
        "#     [-0.9290, -0.0227,  0.6467], one     (x^5)\n",
        "#     [ 0.5691, -2.0627, -3.2411]  step    (x^6)\n",
        "# ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykfq4LulMWSG"
      },
      "source": [
        "### Forcing values to fit between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf5_7By8Ma-V",
        "outputId": "0ebaafa5-f5cc-449f-99b3-db060355a2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.4200, 0.2100],\n",
            "        [0.4500, 0.0000, 0.6800],\n",
            "        [0.3700, 0.3200, 0.0100],\n",
            "        [0.6100, 0.6000, 0.3100],\n",
            "        [0.7000, 0.6500, 0.3200],\n",
            "        [0.2800, 0.9100, 0.9600]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import decimal\n",
        "min_val = small_input_embeddings.min()\n",
        "max_val = small_input_embeddings.max()\n",
        "scaled_embeddings = (small_input_embeddings - min_val) / (max_val - min_val)\n",
        "rounded_embeddings = torch.round(scaled_embeddings * 100) / 100\n",
        "print(rounded_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gYidQzDQAI5"
      },
      "source": [
        "## Simple self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UryRjq28FZ39"
      },
      "source": [
        "### Step 1 - Compute unormalized attention scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6wJC7RJU3c",
        "outputId": "aed35865-3af0-4509-830a-67a68d82906a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dot product of tensor([1.0000, 0.4200, 0.2100], grad_fn=<UnbindBackward0>) against journey tensor([0.4500, 0.0000, 0.6800], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.4500, 0.0000, 0.6800], grad_fn=<UnbindBackward0>) against journey tensor([0.4500, 0.0000, 0.6800], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.3700, 0.3200, 0.0100], grad_fn=<UnbindBackward0>) against journey tensor([0.4500, 0.0000, 0.6800], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.6100, 0.6000, 0.3100], grad_fn=<UnbindBackward0>) against journey tensor([0.4500, 0.0000, 0.6800], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.7000, 0.6500, 0.3200], grad_fn=<UnbindBackward0>) against journey tensor([0.4500, 0.0000, 0.6800], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.2800, 0.9100, 0.9600], grad_fn=<UnbindBackward0>) against journey tensor([0.4500, 0.0000, 0.6800], grad_fn=<SelectBackward0>)\n",
            "tensor([0.5928, 0.6649, 0.1733, 0.4853, 0.5326, 0.7788], grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "query = rounded_embeddings[1]  # 2nd input token is the query)\n",
        "# just allocate a tensor in memory with 6 spaces\n",
        "attn_scores_2 = torch.empty(rounded_embeddings.shape[0])\n",
        "\n",
        "#fill the tensor with the dot products which multiply and sum\n",
        "for i, x_i in enumerate(rounded_embeddings):\n",
        "    print(f'dot product of {x_i} against journey {query}')\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMsw6j8WJsFU"
      },
      "source": [
        "### Step 2 - Normalize the attenton scores to sum up to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PF1wtEJJrnj",
        "outputId": "8d7a6e84-5dfb-41e7-bd6b-60ab65aed437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1731, 0.1861, 0.1138, 0.1555, 0.1630, 0.2085],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Sum: tensor(1.0000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MB0l0Y-a090",
        "outputId": "e3f2f2aa-5b62-487c-d25b-075287f5d6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "torch.Size([2, 3, 4])\n",
            "4\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Fooling around with dimensions on vectors\n",
        "my_tensor = torch.ones(2,3,4) # I always start with the last dimension which is [-1]\n",
        "print(my_tensor)\n",
        "print(my_tensor.shape)\n",
        "print(my_tensor.shape[-1]) # last dimension\n",
        "print(my_tensor.shape[0]) # first dimenson\n",
        "print(my_tensor.shape[1])\n",
        "print(my_tensor.shape[2]) # same as [-1]\n",
        "# print(my_tensor.shape[3]) # IndexError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIYCzERyLwBQ",
        "outputId": "ab5f4058-a16f-4240-b413-73d48bac290a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1731, 0.1861, 0.1138, 0.1555, 0.1630, 0.2085],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Sum: tensor(1., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# using pytorch softmax fucntion\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum()) #100%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsNQPLyNQ9DN"
      },
      "source": [
        "### Step 3 - Compute the context vector $z^{(2)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tba1xAyQ5G2",
        "outputId": "48b6b81a-8d29-4801-e332-554f886b259d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1731238067150116 * tensor([1.0000, 0.4200, 0.2100], grad_fn=<UnbindBackward0>)\n",
            "0.18606702983379364 * tensor([0.4500, 0.0000, 0.6800], grad_fn=<UnbindBackward0>)\n",
            "0.1138073280453682 * tensor([0.3700, 0.3200, 0.0100], grad_fn=<UnbindBackward0>)\n",
            "0.15547841787338257 * tensor([0.6100, 0.6000, 0.3100], grad_fn=<UnbindBackward0>)\n",
            "0.16300925612449646 * tensor([0.7000, 0.6500, 0.3200], grad_fn=<UnbindBackward0>)\n",
            "0.20851415395736694 * tensor([0.2800, 0.9100, 0.9600], grad_fn=<UnbindBackward0>)\n",
            "tensor(1.0000, grad_fn=<AddBackward0>)\n",
            "tensor([0.5663, 0.4981, 0.4646], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "query = rounded_embeddings[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "attn_weights_sum = 0\n",
        "for i,x_i in enumerate(rounded_embeddings):\n",
        "    print(f'{attn_weights_2[i]} * {x_i}')\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "    attn_weights_sum += attn_weights_2[i]\n",
        "\n",
        "print(attn_weights_sum)\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtDPseV2Tgyp"
      },
      "source": [
        "### Get All attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDMWyFwTgmA",
        "outputId": "bf3e2db8-f3a6-49a6-ad93-c1263479d555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2205, 0.5928, 0.5065, 0.9271, 1.0402, 0.8638],\n",
            "        [0.5928, 0.6649, 0.1733, 0.4853, 0.5326, 0.7788],\n",
            "        [0.5065, 0.1733, 0.2394, 0.4208, 0.4702, 0.4044],\n",
            "        [0.9271, 0.4853, 0.4208, 0.8282, 0.9162, 1.0144],\n",
            "        [1.0402, 0.5326, 0.4702, 0.9162, 1.0149, 1.0947],\n",
            "        [0.8638, 0.7788, 0.4044, 1.0144, 1.0947, 1.8281]],\n",
            "       grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(rounded_embeddings):\n",
        "    for j, x_j in enumerate(rounded_embeddings):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvNIm4n3TsBT"
      },
      "source": [
        "We can achive the same but more efficiently via matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaHP2VdZCouE",
        "outputId": "ffa15bad-45f6-4034-aa41-7177d1edccfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n",
            "torch.Size([3, 6])\n",
            "-------------------\n",
            "tensor([[1.0000, 0.4200, 0.2100],\n",
            "        [0.4500, 0.0000, 0.6800],\n",
            "        [0.3700, 0.3200, 0.0100],\n",
            "        [0.6100, 0.6000, 0.3100],\n",
            "        [0.7000, 0.6500, 0.3200],\n",
            "        [0.2800, 0.9100, 0.9600]], grad_fn=<DivBackward0>)\n",
            "-------------------\n",
            "tensor([[1.0000, 0.4500, 0.3700, 0.6100, 0.7000, 0.2800],\n",
            "        [0.4200, 0.0000, 0.3200, 0.6000, 0.6500, 0.9100],\n",
            "        [0.2100, 0.6800, 0.0100, 0.3100, 0.3200, 0.9600]],\n",
            "       grad_fn=<PermuteBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Using matrix transpose (remember is row * column)\n",
        "print(rounded_embeddings.shape)\n",
        "print(rounded_embeddings.T.shape)\n",
        "print('-------------------')\n",
        "print(rounded_embeddings)\n",
        "print('-------------------')\n",
        "print(rounded_embeddings.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MeXFpyjTwRW",
        "outputId": "7fd2930a-3bc1-4183-e56e-649d8a8f82d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2205, 0.5928, 0.5065, 0.9271, 1.0402, 0.8638],\n",
            "        [0.5928, 0.6649, 0.1733, 0.4853, 0.5326, 0.7788],\n",
            "        [0.5065, 0.1733, 0.2394, 0.4208, 0.4702, 0.4044],\n",
            "        [0.9271, 0.4853, 0.4208, 0.8282, 0.9162, 1.0144],\n",
            "        [1.0402, 0.5326, 0.4702, 0.9162, 1.0149, 1.0947],\n",
            "        [0.8638, 0.7788, 0.4044, 1.0144, 1.0947, 1.8281]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = rounded_embeddings @ rounded_embeddings.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbaMTFAmT4jD"
      },
      "source": [
        "Apply softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNxM31OqT3r2",
        "outputId": "6c51b4cf-e0ee-4e6b-9d5b-3f4e4a05b152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2323, 0.1240, 0.1138, 0.1733, 0.1940, 0.1626],\n",
            "        [0.1731, 0.1861, 0.1138, 0.1555, 0.1630, 0.2085],\n",
            "        [0.1898, 0.1360, 0.1453, 0.1743, 0.1831, 0.1714],\n",
            "        [0.1911, 0.1229, 0.1152, 0.1731, 0.1891, 0.2086],\n",
            "        [0.1968, 0.1184, 0.1113, 0.1738, 0.1919, 0.2078],\n",
            "        [0.1317, 0.1209, 0.0832, 0.1531, 0.1659, 0.3453]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# dim=-1 is to apply softmax to the last dimenison, in this case rows\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPREekd0T-_j",
        "outputId": "bc711389-356c-4b51-ae93-6bc781fceede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "       grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "row_0_sum = sum([0.1403, 0.1365, 0.1915, 0.1552, 0.1659, 0.2106])\n",
        "print(row_0_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y40kr-uUM19"
      },
      "source": [
        "### Compute All context vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATCLH64zUMp-",
        "outputId": "e7314c4b-8418-44a9-f9cb-f9c6db6029de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6173, 0.5120, 0.4062],\n",
            "        [0.5663, 0.4981, 0.4646],\n",
            "        [0.5873, 0.5058, 0.4110],\n",
            "        [0.5854, 0.5337, 0.4393],\n",
            "        [0.5898, 0.5364, 0.4378],\n",
            "        [0.5230, 0.5958, 0.5428]], grad_fn=<MmBackward0>)\n",
            "Previous 2nd context vector: tensor([0.5663, 0.4981, 0.4646], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ rounded_embeddings\n",
        "print(all_context_vecs)\n",
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VghIaCejQ6JH"
      },
      "source": [
        "## Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1218PmTRnuh"
      },
      "source": [
        "**Weight parameters** are learned coefficients that define the network connections, while **attention weights** are dynamic, context-specific values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObbzoVnCShzc",
        "outputId": "d7e1f94e-a753-4c05-e3f8-9e0f6db79b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ],
      "source": [
        "x_2 = rounded_embeddings[1] # second input element\n",
        "d_in = rounded_embeddings.shape[1] #depends on the embeddings dimension the input embedding size, dim=3\n",
        "d_out = 2 # the output embedding size, dim=2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "print(W_query)\n",
        "print(W_key)\n",
        "print(W_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TC8hnCwUce2",
        "outputId": "35d7844a-3b17-4978-9411-b33fa9ee3438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1836, 0.8217], grad_fn=<SqueezeBackward4>)\n",
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)\n",
        "\n",
        "# calculate keys and values vector for all inputs\n",
        "keys = rounded_embeddings @ W_key\n",
        "values = rounded_embeddings @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi-rk9d_U3_8",
        "outputId": "1cc225b4-8330-42f3-a24a-f9bee45bd8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4724, grad_fn=<DotBackward0>)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2w0_CCgU030",
        "outputId": "02443b76-ac72-4006-80d1-bf7545518825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5049, 0.4724, 0.2485, 0.6380, 0.6856, 1.2021],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgtjdYYBVrjm"
      },
      "source": [
        "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension,  𝑑𝑘‾‾‾√  (i.e., d_k**0.5):\n",
        "\n",
        "Imagine you have two vectors, and their dot product results in a large value. When this large value is passed through the softmax function, it might dominate the probabilities, making the attention mechanism less sensitive to other relevant parts of the input. Scaling helps to mitigate this issue by preventing any single dot product from becoming overly influential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFekEwTuVWFt",
        "outputId": "7742d4b9-51c0-4cca-c49d-231f6df02297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1448, 0.1576, 0.1573, 0.1993, 0.1977, 0.1433],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[1] # dimension of keys\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzEw5VL0XQqE",
        "outputId": "5ba0bcc8-208d-40a0-ba35-91deab668269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2555, 0.6958], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Dg25JCXccF"
      },
      "source": [
        "### Compact SelfAttention Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPFiRyaKX0Lb"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzWub2DXvao"
      },
      "source": [
        "- We can streamline the implementation above using PyTorch's Linear layers instead of torch random, which are equivalent to a matrix multiplication if we disable the bias units\n",
        "- Another big advantage of using `nn.Linear` over our manual `nn.Parameter(torch.rand(...)` approach is that `nn.Linear` has a preferred weight initialization scheme, which leads to more stable model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Yx74nUgLfNJB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "8O5vlV-nXw3p"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2mJv-ETX-g3",
        "outputId": "a8189a9d-f456-4b5f-8611-44ac4079c73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0079,  0.1668],\n",
            "        [-0.0053,  0.1702],\n",
            "        [-0.0075,  0.1675],\n",
            "        [-0.0079,  0.1668],\n",
            "        [-0.0081,  0.1665],\n",
            "        [-0.0073,  0.1675]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(rounded_embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLpyQ2keYaxo"
      },
      "source": [
        "### Hiding futer words with causal attention (one step back)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd9vAnBTYjJp",
        "outputId": "396060b5-9c61-452f-8384-8fe89a01a177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1694, 0.1863, 0.1518, 0.1606, 0.1617, 0.1702],\n",
            "        [0.1721, 0.1817, 0.1576, 0.1624, 0.1632, 0.1630],\n",
            "        [0.1672, 0.1752, 0.1594, 0.1640, 0.1644, 0.1698],\n",
            "        [0.1695, 0.1866, 0.1516, 0.1605, 0.1616, 0.1702],\n",
            "        [0.1696, 0.1885, 0.1502, 0.1599, 0.1611, 0.1706],\n",
            "        [0.1732, 0.1988, 0.1449, 0.1571, 0.1587, 0.1672]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Reuse data from previous section\n",
        "queries = sa_v2.W_query(rounded_embeddings)\n",
        "keys = sa_v2.W_key(rounded_embeddings)\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKX_phRVdlvv"
      },
      "source": [
        "Applying negative infinity effectively zeros out the probabilities for these future tokens in the subsequent softmax calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmtxjas9cwln",
        "outputId": "ad244e2b-f49c-4107-ed73-32240b45bf75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1598,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.1269, 0.2032,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.0697, 0.1352, 0.0021,   -inf,   -inf,   -inf],\n",
            "        [0.1624, 0.2988, 0.0044, 0.0857,   -inf,   -inf],\n",
            "        [0.1770, 0.3265, 0.0048, 0.0939, 0.1040,   -inf],\n",
            "        [0.2582, 0.4533, 0.0061, 0.1200, 0.1349, 0.2085]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[-1]\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5mLgJvOdGAr",
        "outputId": "f0bab9b7-9802-42fc-c251-3e09cf31c31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4865, 0.5135, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3490, 0.3177, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2536, 0.2793, 0.2268, 0.2402, 0.0000, 0.0000],\n",
            "        [0.2045, 0.2273, 0.1811, 0.1929, 0.1942, 0.0000],\n",
            "        [0.1732, 0.1988, 0.1449, 0.1571, 0.1587, 0.1672]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-XqXxKcYjbe"
      },
      "source": [
        "### Masking additional attention weights with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGyeNovHe97q",
        "outputId": "290032b4-1cd0-408b-ed60-9bbdb240bc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n",
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6354, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5586, 0.0000, 0.4805, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4546, 0.3621, 0.3857, 0.3885, 0.0000],\n",
            "        [0.3464, 0.3976, 0.0000, 0.0000, 0.3175, 0.3345]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6, 6) # create a matrix of ones\n",
        "\n",
        "print(dropout(example))\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9schnK0fRte"
      },
      "source": [
        "### Causal Attention Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8GxHW4IfRVU"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        print(b)\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "9eoq6pVwfyzu",
        "outputId": "148e81f1-0a37-4a74-dcdf-af97acb2d151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CausalAttention' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-069c3951b853>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcontext_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCausalAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CausalAttention' is not defined"
          ]
        }
      ],
      "source": [
        "# torch.manual_seed(123)\n",
        "\n",
        "# batch = torch.stack((rounded_embeddings, rounded_embeddings), dim=0)\n",
        "# print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
        "\n",
        "# context_length = batch.shape[1]\n",
        "# ca = CausalAttention(d_in, d_out, context_lenght, 0.0)\n",
        "\n",
        "# context_vecs = ca(batch)\n",
        "\n",
        "# print(context_vecs)\n",
        "# print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UydLlF4rj0hf"
      },
      "source": [
        "## Self Attention multi-head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0f3nXA9ne-Q"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAZxS6r3nWzN",
        "outputId": "bcf0d4d0-9244-43fe-b8d3-5239af1d7621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "torch.Size([2, 6, 2, 1]) vs torch.Size([2, 2, 6, 1])\n",
            "--------------------\n",
            "tensor([[[0.2699, 0.4130],\n",
            "         [0.2399, 0.4178],\n",
            "         [0.2289, 0.4308],\n",
            "         [0.2306, 0.4467],\n",
            "         [0.2335, 0.4329],\n",
            "         [0.2335, 0.4581]],\n",
            "\n",
            "        [[0.2699, 0.4130],\n",
            "         [0.2399, 0.4178],\n",
            "         [0.2289, 0.4308],\n",
            "         [0.2306, 0.4467],\n",
            "         [0.2335, 0.4329],\n",
            "         [0.2335, 0.4581]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape #b is for batches\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        print('--------------------')\n",
        "        print(f'{keys.shape} vs {keys.transpose(1,2).shape}')\n",
        "        print('--------------------')\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "123g1w8Vz_0T",
        "KiqznPKxE2-J",
        "kQyY8us8kjnU",
        "sdqVKwHw9WOr",
        "e5oaJW523vt2",
        "b0nYzc5uBMkF",
        "cXjmjDeM6z-5",
        "t-qc5coF7P9c",
        "scsHf8OgJiSX",
        "N3bdp26B93oU",
        "JGLUALUt968u",
        "SC3hdDBCQyxj",
        "4gYidQzDQAI5",
        "UryRjq28FZ39"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}