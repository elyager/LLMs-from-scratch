{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elyager/LLMs-from-scratch/blob/main/LLM_from_scratch_personal_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123g1w8Vz_0T"
      },
      "source": [
        "### Requiriments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCSj5qDn9bP-",
        "outputId": "b73f9047-730f-446a-e72f-a88c5ce77bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqznPKxE2-J"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQd3ILu5Czb"
      },
      "source": [
        "- **vocab_size:** es el tamaño del vocabulario en tokens únicos disponibles+extensiones. En nuestro caso dado por el tokenizador creado con BPE.\n",
        "- **output_dim:** el número de dimensiones de cada token. Las dimensiones describen a una palabra o concepto. Más dimensiones capturan más detalles.\n",
        "- **max_length:** es la máxima logitud de tokens por secuencia.\n",
        "- **batch_size:** es cuantas secuencias (muestras de texto) tiene cada batch. Larger batch sizes can speed up training but might require more memory.\n",
        "- **stride:** el tamaño de la zancada en tokens, cuantos tokens salta para la siguiente secuencia, esto determina que tanto se empalma una secuencia con otra. Ensure the model sees the context of each token multiple times during training, helping it learn better relationships between words.\n",
        "- **shuffle:** determina si le da un orden aleatorio a las secuencias para que sea random en cada epoc. Prevents the model from learning patterns based on the order of data presentation. It helps generalize learning and avoid overfitting to a specific data order.\n",
        "-**drop_last:** indica si se debe descartar el último batch cuando no cumple con el número de muestras establecido en max_lenght. Si se tiene un set de datos pequeño es mejor no hacer drop.\n",
        "-**num_workers:** es el número de procesos, a mayor número más rápidez.\n",
        "-**attention score:** determina qué tan \"similar\" es una pababra con la otra a través de dot product que es un tipo de similarity function. Mayor atention score mayor similitud entre los números.\n",
        "--**attention weight:** es la versión normalizada a través de softmax de los attention scores.\n",
        "-**context vector:** es un embedding vector pero que tiene todo el contexto del resto de input vectors. Se obtiene sumando todos los attention weights del inpute secuence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZbR6dXofBXJx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN__3TwCBXiO"
      },
      "source": [
        "# Chapter 1 & 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyY8us8kjnU",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load the text for training (our corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLL4xfSX9C0p",
        "outputId": "11853d89-df3c-49dd-b814-a218014109a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqVKwHw9WOr",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### BytePair Encoding (BPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmSakIi9grv",
        "outputId": "b283c2d9-7c72-4652-be25-17e4e0c59d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoAAtvXt9r53",
        "outputId": "6311d0e7-7898-4219-d43a-6046412037c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I |-| H |-|AD |-| always |-| thought |-| Jack |-| G |-|is |-|burn |-| rather\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n",
            "\n",
            " Total of tokens: 5146\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
        "enc_text.append(tokenizer.eot_token)\n",
        "\n",
        "# First 10 tokens from raw_text\n",
        "first_10_token_ids = enc_text[:10]\n",
        "decoded_tokens = [tokenizer.decode([token_id]) for token_id in first_10_token_ids]\n",
        "delimited_tokens = ' |-|'.join(decoded_tokens)\n",
        "print(delimited_tokens)\n",
        "print(enc_text[:10])\n",
        "print(f'\\n Total of tokens: {len(enc_text)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5oaJW523vt2",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Dataset loader (creating tokenIDs for inputs and targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o6B2KxAkAbLC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        token_ids.append(tokenizer.eot_token)\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# dataset = GPTDatasetV1(raw_text, tokenizer, max_length=4, stride=1)\n",
        "# print(dataset.input_ids)\n",
        "# print(dataset.target_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KXlE1wG6AsOk"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0nYzc5uBMkF"
      },
      "source": [
        "### Use DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gIJnkOxAvfj",
        "outputId": "d9f11e95-b148-4ea0-ed3c-cb761831f8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "# for batch in dataloader:\n",
        "#     input, target = batch\n",
        "#     print(input, target)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch) # input and target\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch) # input and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Oyu9uJEay2",
        "outputId": "8a39c401-f87d-42fc-b74d-8d779b6d7df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "Inputs:\n",
            " I HAD always\n",
            "\n",
            "Targets:\n",
            "  HAD always thought\n",
            "Inputs:\n",
            " tensor([[  287,   262,  6001,   286],\n",
            "        [  465, 13476,    11,   339],\n",
            "        [  550,  5710,   465, 12036],\n",
            "        [   11,  6405,   257,  5527],\n",
            "        [27075,    11,   290,  4920],\n",
            "        [ 2241,   287,   257,  4489],\n",
            "        [   64,   319,   262, 34686],\n",
            "        [41976,    13,   357, 10915]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  262,  6001,   286,   465],\n",
            "        [13476,    11,   339,   550],\n",
            "        [ 5710,   465, 12036,    11],\n",
            "        [ 6405,   257,  5527, 27075],\n",
            "        [   11,   290,  4920,  2241],\n",
            "        [  287,   257,  4489,    64],\n",
            "        [  319,   262, 34686, 41976],\n",
            "        [   13,   357, 10915,   314]])\n",
            "Inputs:\n",
            "  in the height of\n",
            "\n",
            "Targets:\n",
            "  the height of his\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "# First batch\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
        "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))\n",
        "\n",
        "# Second batch\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
        "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXjmjDeM6z-5"
      },
      "source": [
        "### Create our token embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax4UNR13E4wB",
        "outputId": "63ee9b3d-34c9-4b13-92ff-74188bbc8bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257, 3])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0914, -0.9365,  0.7240],\n",
            "        [ 0.2457,  0.1994, -0.9671],\n",
            "        [-0.6384, -1.9128,  1.3959],\n",
            "        ...,\n",
            "        [ 0.1865, -1.0996,  0.0158],\n",
            "        [-0.3695, -1.5027, -2.1622],\n",
            "        [-0.5088,  0.7261,  1.1936]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 3 # 256 is a more common starting point\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(token_embedding_layer.weight.shape)\n",
        "print(token_embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaPfzy6d631b"
      },
      "source": [
        "#### Load our dataset to get the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuqLpfanGGBp",
        "outputId": "d83918c7-fc13-4d0c-ed65-68dec92af29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Token IDs:\\n\",  inputs) # we take the first batch and  ignore the targets for now\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-qc5coF7P9c"
      },
      "source": [
        "### Create token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ziD4fWmIUMy",
        "outputId": "cd19a1ba-a233-43fc-80c9-88fc39c95828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 3])\n",
            "tensor([[[-0.8138, -0.7311,  1.2164],\n",
            "         [-0.8966, -0.8683,  1.7272],\n",
            "         [-0.0507, -0.7772,  0.4739],\n",
            "         [-0.4029,  0.7053,  0.4074]],\n",
            "\n",
            "        [[ 0.2102, -0.9068,  0.8955],\n",
            "         [-0.3159, -1.1545, -0.6932],\n",
            "         [-0.7584, -0.7269,  0.4727],\n",
            "         [-1.9021, -2.1596, -1.0677]],\n",
            "\n",
            "        [[-0.1855,  0.4751,  0.8388],\n",
            "         [ 0.1405, -1.5706,  0.7147],\n",
            "         [-0.5403, -0.6365, -0.0687],\n",
            "         [-0.9725, -0.0561,  0.2927]],\n",
            "\n",
            "        [[-0.7529, -0.0139, -0.9845],\n",
            "         [ 1.4724, -0.6392,  1.7239],\n",
            "         [ 1.4560, -1.2420, -0.0319],\n",
            "         [-0.5403, -0.6365, -0.0687]],\n",
            "\n",
            "        [[ 1.8129,  1.0460,  2.0427],\n",
            "         [ 0.5189,  0.9129, -0.7249],\n",
            "         [ 0.8845, -1.0674, -2.6862],\n",
            "         [ 1.4724, -0.6392,  1.7239]],\n",
            "\n",
            "        [[ 0.5230, -0.2953, -1.4464],\n",
            "         [-1.2002,  1.1369, -0.0891],\n",
            "         [-0.5462,  1.8767, -0.4185],\n",
            "         [-1.0630, -1.2027,  1.0012]],\n",
            "\n",
            "        [[-1.5859, -0.3160, -0.4833],\n",
            "         [-1.2017, -0.4922, -0.2306],\n",
            "         [ 0.2207, -0.0123, -0.1116],\n",
            "         [-1.2837, -0.1873, -1.2614]],\n",
            "\n",
            "        [[ 0.2207, -0.0123, -0.1116],\n",
            "         [ 0.4112, -0.9903, -0.7535],\n",
            "         [-0.2834,  0.6693, -0.2733],\n",
            "         [-0.9154,  1.1384, -0.1044]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "# each token now has the assigned number of dimentions instead of being a single token ID\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(token_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scsHf8OgJiSX"
      },
      "source": [
        "### Create absolute positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "549fxfhkJh_V",
        "outputId": "443bddd4-1cfd-43e5-b9a6-e532b0c5f8ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n",
            "tensor([[-0.4791,  0.4190, -2.3272],\n",
            "        [-0.4893, -0.4136,  0.2769],\n",
            "        [-1.1482,  0.3328,  1.1950],\n",
            "        [ 0.6695,  0.2243,  0.1151]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "\n",
        "# # [0, 1, 2, 3] \"column\" position is the position of each word on each sequence of 4 context_length\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "# # uncomment & execute the following line to see how the embeddings look like\n",
        "print(pos_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3bdp26B93oU"
      },
      "source": [
        "### Create input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F00jaZjOKPpw",
        "outputId": "1cf82ffb-4086-4852-c7f5-18e4cffc54c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 3]) + torch.Size([4, 3]) = torch.Size([8, 4, 3])\n",
            "tensor([[[-1.2929, -0.3120, -1.1107],\n",
            "         [-1.3859, -1.2819,  2.0041],\n",
            "         [-1.1990, -0.4444,  1.6688],\n",
            "         [ 0.2666,  0.9296,  0.5224]],\n",
            "\n",
            "        [[-0.2689, -0.4878, -1.4316],\n",
            "         [-0.8052, -1.5681, -0.4163],\n",
            "         [-1.9066, -0.3941,  1.6677],\n",
            "         [-1.2326, -1.9353, -0.9527]],\n",
            "\n",
            "        [[-0.6646,  0.8941, -1.4884],\n",
            "         [-0.3488, -1.9842,  0.9916],\n",
            "         [-1.6885, -0.3037,  1.1263],\n",
            "         [-0.3030,  0.1682,  0.4077]],\n",
            "\n",
            "        [[-1.2321,  0.4051, -3.3116],\n",
            "         [ 0.9831, -1.0528,  2.0008],\n",
            "         [ 0.3078, -0.9092,  1.1631],\n",
            "         [ 0.1292, -0.4122,  0.0464]],\n",
            "\n",
            "        [[ 1.3338,  1.4651, -0.2844],\n",
            "         [ 0.0297,  0.4993, -0.4481],\n",
            "         [-0.2637, -0.7346, -1.4913],\n",
            "         [ 2.1419, -0.4150,  1.8390]],\n",
            "\n",
            "        [[ 0.0439,  0.1237, -3.7736],\n",
            "         [-1.6895,  0.7234,  0.1877],\n",
            "         [-1.6944,  2.2095,  0.7765],\n",
            "         [-0.3935, -0.9784,  1.1162]],\n",
            "\n",
            "        [[-2.0650,  0.1030, -2.8104],\n",
            "         [-1.6910, -0.9058,  0.0462],\n",
            "         [-0.9275,  0.3204,  1.0834],\n",
            "         [-0.6142,  0.0370, -1.1464]],\n",
            "\n",
            "        [[-0.2584,  0.4067, -2.4388],\n",
            "         [-0.0781, -1.4039, -0.4767],\n",
            "         [-1.4316,  1.0021,  0.9216],\n",
            "         [-0.2459,  1.3627,  0.0107]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(input_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGLUALUt968u"
      },
      "source": [
        "### What about more dimmension?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SRFPsblZ-CfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38db3de-b6c0-45cf-c067-1b382baa404a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "tensor([[[ 1.3295e+00, -2.6935e-01,  2.8294e-01,  ...,  7.5636e-01,\n",
            "          -2.6170e+00, -1.4374e+00],\n",
            "         [ 9.8064e-01,  1.8127e-01, -8.6219e-01,  ..., -4.2655e-01,\n",
            "          -8.7672e-01,  6.3201e-01],\n",
            "         [ 6.1898e-01,  9.3566e-02, -3.4799e-01,  ...,  6.5005e-01,\n",
            "           8.5892e-01,  1.4344e+00],\n",
            "         [-1.4821e-02, -1.4379e+00, -3.6418e-01,  ..., -2.7496e-01,\n",
            "          -1.1600e-01,  1.0902e+00]],\n",
            "\n",
            "        [[-4.4345e-01, -1.3204e-03,  1.2373e+00,  ..., -8.0514e-01,\n",
            "          -8.6535e-02, -3.1204e-01],\n",
            "         [ 1.6229e+00,  1.2113e+00,  1.2216e+00,  ...,  1.1292e+00,\n",
            "           1.5319e+00,  2.4835e-02],\n",
            "         [-2.3544e-01,  7.7411e-02,  1.5386e-01,  ...,  4.1874e-01,\n",
            "          -3.5889e-01,  3.7214e-01],\n",
            "         [ 5.3323e-02, -1.2211e+00,  4.0893e-01,  ...,  2.3007e+00,\n",
            "          -1.2084e+00,  1.5010e+00]],\n",
            "\n",
            "        [[ 1.2954e-01,  1.1265e+00,  8.4160e-01,  ..., -1.7333e+00,\n",
            "           2.1296e-01, -3.5150e-01],\n",
            "         [ 3.8380e-01,  2.4606e-01, -4.7529e-01,  ...,  3.2239e-01,\n",
            "           7.2868e-01,  1.9898e+00],\n",
            "         [-3.6479e-01, -9.2143e-01, -6.0478e-01,  ...,  5.1310e-01,\n",
            "          -1.3043e-01,  1.0484e+00],\n",
            "         [-1.2289e+00,  8.0532e-01, -5.1247e-01,  ...,  9.1817e-01,\n",
            "          -4.7896e-01, -9.3434e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.6340e-01, -9.9696e-01,  6.1298e-01,  ..., -1.7890e-01,\n",
            "           7.1507e-01,  1.9359e-01],\n",
            "         [-1.3524e+00,  5.3624e-01, -1.3129e+00,  ...,  1.7320e-01,\n",
            "           2.3317e-01, -2.8036e-01],\n",
            "         [-2.0558e-01,  1.2650e+00,  7.0491e-01,  ..., -5.7126e-01,\n",
            "           2.4054e-02, -3.6679e-02],\n",
            "         [ 7.6800e-01, -5.3295e-01, -1.6579e+00,  ..., -1.0222e-01,\n",
            "          -9.7766e-01,  3.8351e-01]],\n",
            "\n",
            "        [[ 2.4302e+00,  1.6929e-01,  9.9762e-01,  ...,  5.8300e-01,\n",
            "          -7.0890e-01,  8.2195e-01],\n",
            "         [-1.0216e+00, -1.7084e+00, -1.1325e+00,  ...,  1.0653e+00,\n",
            "           4.5699e-02,  3.8462e-01],\n",
            "         [ 5.1193e-01, -9.9454e-01, -3.5477e-01,  ..., -1.6618e+00,\n",
            "          -1.6509e+00, -1.6238e-01],\n",
            "         [ 2.0287e+00,  3.6613e-01,  3.7758e-01,  ...,  4.1303e-01,\n",
            "          -5.4081e-01, -5.0535e-01]],\n",
            "\n",
            "        [[ 5.1193e-01, -9.9454e-01, -3.5477e-01,  ..., -1.6618e+00,\n",
            "          -1.6509e+00, -1.6238e-01],\n",
            "         [ 2.3877e+00,  4.8585e-01, -9.9679e-01,  ..., -2.1859e+00,\n",
            "           1.7271e+00,  4.1440e-01],\n",
            "         [ 1.6048e+00, -9.7611e-01,  2.0036e+00,  ...,  1.6403e+00,\n",
            "          -1.2087e+00,  1.2344e+00],\n",
            "         [ 3.8749e-01, -2.0301e+00,  2.6084e-02,  ...,  6.8326e-01,\n",
            "           4.5441e-01,  6.2981e-01]]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([4, 256])\n",
            "tensor([[ 1.1230,  0.6140,  1.6109,  ...,  0.1440, -0.6858,  0.7300],\n",
            "        [ 0.6408,  0.4700, -0.1715,  ...,  0.4798,  1.2691,  0.5870],\n",
            "        [ 1.6299, -0.7688, -1.1108,  ..., -0.2801,  1.0797, -0.6712],\n",
            "        [-0.8292,  0.2850,  0.5013,  ..., -0.4615,  2.0363, -0.4589]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([8, 4, 256]) + torch.Size([4, 256]) = torch.Size([8, 4, 256])\n",
            "tensor([[[ 2.4526,  0.3447,  1.8939,  ...,  0.9004, -3.3029, -0.7075],\n",
            "         [ 1.6215,  0.6513, -1.0337,  ...,  0.0532,  0.3924,  1.2190],\n",
            "         [ 2.2488, -0.6752, -1.4588,  ...,  0.3699,  1.9386,  0.7632],\n",
            "         [-0.8441, -1.1529,  0.1371,  ..., -0.7365,  1.9203,  0.6313]],\n",
            "\n",
            "        [[ 0.6796,  0.6127,  2.8482,  ..., -0.6611, -0.7724,  0.4179],\n",
            "         [ 2.2638,  1.6813,  1.0501,  ...,  1.6090,  2.8011,  0.6119],\n",
            "         [ 1.3944, -0.6914, -0.9570,  ...,  0.1386,  0.7208, -0.2991],\n",
            "         [-0.7759, -0.9361,  0.9102,  ...,  1.8392,  0.8280,  1.0421]],\n",
            "\n",
            "        [[ 1.2526,  1.7405,  2.4525,  ..., -1.5893, -0.4729,  0.3785],\n",
            "         [ 1.0246,  0.7161, -0.6468,  ...,  0.8022,  1.9978,  2.5769],\n",
            "         [ 1.2651, -1.6902, -1.7156,  ...,  0.2330,  0.9492,  0.3771],\n",
            "         [-2.0582,  1.0903, -0.0112,  ...,  0.4567,  1.5574, -1.3932]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.0864, -0.3830,  2.2239,  ..., -0.0349,  0.0292,  0.9236],\n",
            "         [-0.7115,  1.0063, -1.4844,  ...,  0.6530,  1.5023,  0.3067],\n",
            "         [ 1.4243,  0.4962, -0.4059,  ..., -0.8514,  1.1037, -0.7079],\n",
            "         [-0.0612, -0.2479, -1.1566,  ..., -0.5637,  1.0587, -0.0753]],\n",
            "\n",
            "        [[ 3.5532,  0.7833,  2.6086,  ...,  0.7270, -1.3947,  1.5519],\n",
            "         [-0.3807, -1.2384, -1.3040,  ...,  1.5451,  1.3148,  0.9717],\n",
            "         [ 2.1418, -1.7633, -1.4656,  ..., -1.9420, -0.5712, -0.8336],\n",
            "         [ 1.1995,  0.6511,  0.8789,  ..., -0.0485,  1.4955, -0.9642]],\n",
            "\n",
            "        [[ 1.6350, -0.3805,  1.2562,  ..., -1.5178, -2.3368,  0.5676],\n",
            "         [ 3.0285,  0.9559, -1.1683,  ..., -1.7061,  2.9962,  1.0014],\n",
            "         [ 3.2347, -1.7449,  0.8928,  ...,  1.3602, -0.1290,  0.5632],\n",
            "         [-0.4417, -1.7451,  0.5274,  ...,  0.2217,  2.4908,  0.1710]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def more_more_more_dimensions(dim):\n",
        "  vocab_size = 50257\n",
        "  output_dim = dim\n",
        "\n",
        "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "  token_embeddings = token_embedding_layer(inputs)\n",
        "  print(token_embeddings.shape)\n",
        "  print(token_embeddings)\n",
        "\n",
        "  pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "  pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "  print(pos_embeddings.shape)\n",
        "  print(pos_embeddings)\n",
        "\n",
        "\n",
        "  input_embeddings = token_embeddings + pos_embeddings\n",
        "  print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "  print(input_embeddings)\n",
        "\n",
        "more_more_more_dimensions(256)  #256 is a more common starting point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTcK0ieCyHO"
      },
      "source": [
        "# Chapter 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC3hdDBCQyxj"
      },
      "source": [
        "## Preparing data to work with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL4kUkq1FNVy"
      },
      "source": [
        "### Get input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DUVjip9OAjSn"
      },
      "outputs": [],
      "source": [
        "def get_input_embeddings(my_raw_text):\n",
        "  dataloader = create_dataloader_v1(my_raw_text, batch_size=1, max_length=6, stride=6, shuffle=False)\n",
        "\n",
        "  data_iter = iter(dataloader)\n",
        "  # First and only batch\n",
        "  inputs, targets = next(data_iter)\n",
        "  # print(inputs)\n",
        "  # print(targets)  # ignore the targets\n",
        "\n",
        "  vocab_size = 50257\n",
        "  output_dim = 3\n",
        "\n",
        "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "  token_embeddings = token_embedding_layer(inputs)\n",
        "  context_length = 6\n",
        "  pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "  pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "  input_embeddings = token_embeddings + pos_embeddings\n",
        "  # print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "  # print(input_embeddings)\n",
        "  return input_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQzrouXuGICO",
        "outputId": "d4d545be-f8a9-4d30-8faa-1eb7019aa528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.9353,  2.4711, -2.9080],\n",
            "        [ 0.0640,  0.8755, -1.3891],\n",
            "        [-0.0383,  0.5499, -3.9638],\n",
            "        [ 0.9477, -0.1366, -0.3135],\n",
            "        [-0.1208,  0.2234,  1.2193],\n",
            "        [ 1.2950,  0.7151, -1.4663]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "my_raw_text = \"Your journey starts with one step\"\n",
        "small_input_embeddings = get_input_embeddings(my_raw_text)[0]\n",
        "print(small_input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FkdLF-xTIFdB"
      },
      "outputs": [],
      "source": [
        "# tensor([\n",
        "#     [ 0.4113,  1.3397, -1.2234], Your    (x^1)\n",
        "#     [-1.8881, -0.0679, -1.1267], journey (x^2)\n",
        "#     [-0.2323, -2.2089, -1.6685], starts  (x^3)\n",
        "#     [ 0.5615,  1.2698,  2.5768], with    (x^4)\n",
        "#     [-0.9290, -0.0227,  0.6467], one     (x^5)\n",
        "#     [ 0.5691, -2.0627, -3.2411]  step    (x^6)\n",
        "# ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykfq4LulMWSG"
      },
      "source": [
        "### Forcing values to fit between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf5_7By8Ma-V",
        "outputId": "91d48461-15ee-4a62-a4f2-8380c76390d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.9300, 0.1500],\n",
            "        [0.5800, 0.7000, 0.3700],\n",
            "        [0.5700, 0.6500, 0.0000],\n",
            "        [0.7100, 0.5500, 0.5300],\n",
            "        [0.5600, 0.6100, 0.7500],\n",
            "        [0.7600, 0.6800, 0.3600]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import decimal\n",
        "min_val = small_input_embeddings.min()\n",
        "max_val = small_input_embeddings.max()\n",
        "scaled_embeddings = (small_input_embeddings - min_val) / (max_val - min_val)\n",
        "rounded_embeddings = torch.round(scaled_embeddings * 100) / 100\n",
        "print(rounded_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gYidQzDQAI5"
      },
      "source": [
        "## Simple self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UryRjq28FZ39"
      },
      "source": [
        "### Step 1 - Compute unormalized attention scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6wJC7RJU3c",
        "outputId": "bc9823f6-df83-4250-9657-a13f5c1c8582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dot product of tensor([1.0000, 0.9300, 0.1500], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.7000, 0.3700], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.5800, 0.7000, 0.3700], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.7000, 0.3700], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.5700, 0.6500, 0.0000], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.7000, 0.3700], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.7100, 0.5500, 0.5300], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.7000, 0.3700], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.5600, 0.6100, 0.7500], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.7000, 0.3700], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.7600, 0.6800, 0.3600], grad_fn=<UnbindBackward0>) against journey tensor([0.5800, 0.7000, 0.3700], grad_fn=<SelectBackward0>)\n",
            "tensor([1.2865, 0.9633, 0.7856, 0.9929, 1.0293, 1.0500], grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "query = rounded_embeddings[1]  # 2nd input token is the query)\n",
        "# just allocate a tensor in memory with 6 spaces\n",
        "attn_scores_2 = torch.empty(rounded_embeddings.shape[0])\n",
        "\n",
        "#fill the tensor with the dot products which multiply and sum\n",
        "for i, x_i in enumerate(rounded_embeddings):\n",
        "    print(f'dot product of {x_i} against journey {query}')\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMsw6j8WJsFU"
      },
      "source": [
        "### Step 2 - Normalize the attenton scores to sum up to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PF1wtEJJrnj",
        "outputId": "53442b94-9df1-45a1-a79d-5202ad7718e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.2156, 0.1561, 0.1307, 0.1608, 0.1667, 0.1702],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Sum: tensor(1.0000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MB0l0Y-a090",
        "outputId": "4b5bbdd9-2468-4bed-e71c-27053c676805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "torch.Size([2, 3, 4])\n",
            "4\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Fooling around with dimensions on vectors\n",
        "my_tensor = torch.ones(2,3,4) # I always start with the last dimension which is [-1]\n",
        "print(my_tensor)\n",
        "print(my_tensor.shape)\n",
        "print(my_tensor.shape[-1]) # last dimension\n",
        "print(my_tensor.shape[0]) # first dimenson\n",
        "print(my_tensor.shape[1])\n",
        "print(my_tensor.shape[2]) # same as [-1]\n",
        "# print(my_tensor.shape[3]) # IndexError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIYCzERyLwBQ",
        "outputId": "1b4c9a58-996c-4211-b393-cd28e8545174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.2156, 0.1561, 0.1307, 0.1608, 0.1667, 0.1702],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Sum: tensor(1., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# using pytorch softmax fucntion\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum()) #100%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsNQPLyNQ9DN"
      },
      "source": [
        "### Step 3 - Compute the context vector $z^{(2)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tba1xAyQ5G2",
        "outputId": "383f2a24-76a0-4a14-f408-7157c22ce565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21561072766780853 * tensor([1.0000, 0.9300, 0.1500], grad_fn=<UnbindBackward0>)\n",
            "0.15606530010700226 * tensor([0.5800, 0.7000, 0.3700], grad_fn=<UnbindBackward0>)\n",
            "0.1306568682193756 * tensor([0.5700, 0.6500, 0.0000], grad_fn=<UnbindBackward0>)\n",
            "0.16075389087200165 * tensor([0.7100, 0.5500, 0.5300], grad_fn=<UnbindBackward0>)\n",
            "0.16671313345432281 * tensor([0.5600, 0.6100, 0.7500], grad_fn=<UnbindBackward0>)\n",
            "0.17020007967948914 * tensor([0.7600, 0.6800, 0.3600], grad_fn=<UnbindBackward0>)\n",
            "tensor(1., grad_fn=<AddBackward0>)\n",
            "tensor([0.7174, 0.7005, 0.3616], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "query = rounded_embeddings[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "attn_weights_sum = 0\n",
        "for i,x_i in enumerate(rounded_embeddings):\n",
        "    print(f'{attn_weights_2[i]} * {x_i}')\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "    attn_weights_sum += attn_weights_2[i]\n",
        "\n",
        "print(attn_weights_sum)\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtDPseV2Tgyp"
      },
      "source": [
        "### Get All attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDMWyFwTgmA",
        "outputId": "99baf882-e54b-4594-8993-fafd0dd3653c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.8874, 1.2865, 1.1745, 1.3010, 1.2398, 1.4464],\n",
            "        [1.2865, 0.9633, 0.7856, 0.9929, 1.0293, 1.0500],\n",
            "        [1.1745, 0.7856, 0.7474, 0.7622, 0.7157, 0.8752],\n",
            "        [1.3010, 0.9929, 0.7622, 1.0875, 1.1306, 1.1044],\n",
            "        [1.2398, 1.0293, 0.7157, 1.1306, 1.2482, 1.1104],\n",
            "        [1.4464, 1.0500, 0.8752, 1.1044, 1.1104, 1.1696]],\n",
            "       grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(rounded_embeddings):\n",
        "    for j, x_j in enumerate(rounded_embeddings):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvNIm4n3TsBT"
      },
      "source": [
        "We can achive the same but more efficiently via matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaHP2VdZCouE",
        "outputId": "ef5969da-1655-4554-8ab0-7de13f92c66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n",
            "torch.Size([3, 6])\n",
            "-------------------\n",
            "tensor([[1.0000, 0.9300, 0.1500],\n",
            "        [0.5800, 0.7000, 0.3700],\n",
            "        [0.5700, 0.6500, 0.0000],\n",
            "        [0.7100, 0.5500, 0.5300],\n",
            "        [0.5600, 0.6100, 0.7500],\n",
            "        [0.7600, 0.6800, 0.3600]], grad_fn=<DivBackward0>)\n",
            "-------------------\n",
            "tensor([[1.0000, 0.5800, 0.5700, 0.7100, 0.5600, 0.7600],\n",
            "        [0.9300, 0.7000, 0.6500, 0.5500, 0.6100, 0.6800],\n",
            "        [0.1500, 0.3700, 0.0000, 0.5300, 0.7500, 0.3600]],\n",
            "       grad_fn=<PermuteBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Using matrix transpose (remember is row * column)\n",
        "print(rounded_embeddings.shape)\n",
        "print(rounded_embeddings.T.shape)\n",
        "print('-------------------')\n",
        "print(rounded_embeddings)\n",
        "print('-------------------')\n",
        "print(rounded_embeddings.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MeXFpyjTwRW",
        "outputId": "6d1afaf4-3064-482f-9d50-814504bd584e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.8874, 1.2865, 1.1745, 1.3010, 1.2398, 1.4464],\n",
            "        [1.2865, 0.9633, 0.7856, 0.9929, 1.0293, 1.0500],\n",
            "        [1.1745, 0.7856, 0.7474, 0.7622, 0.7157, 0.8752],\n",
            "        [1.3010, 0.9929, 0.7622, 1.0875, 1.1306, 1.1044],\n",
            "        [1.2398, 1.0293, 0.7157, 1.1306, 1.2482, 1.1104],\n",
            "        [1.4464, 1.0500, 0.8752, 1.1044, 1.1104, 1.1696]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = rounded_embeddings @ rounded_embeddings.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbaMTFAmT4jD"
      },
      "source": [
        "Apply softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNxM31OqT3r2",
        "outputId": "bf7c0b92-0bdf-44c9-c21d-1c6acc8f911d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2658, 0.1458, 0.1303, 0.1479, 0.1391, 0.1710],\n",
            "        [0.2156, 0.1561, 0.1307, 0.1608, 0.1667, 0.1702],\n",
            "        [0.2291, 0.1553, 0.1494, 0.1517, 0.1448, 0.1698],\n",
            "        [0.2087, 0.1534, 0.1218, 0.1686, 0.1760, 0.1715],\n",
            "        [0.1928, 0.1562, 0.1142, 0.1729, 0.1945, 0.1694],\n",
            "        [0.2262, 0.1522, 0.1278, 0.1607, 0.1617, 0.1715]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# dim=-1 is to apply softmax to the last dimenison, in this case rows\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPREekd0T-_j",
        "outputId": "6d7008fd-addf-45a1-d6dc-185227c99b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "       grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "row_0_sum = sum([0.1403, 0.1365, 0.1915, 0.1552, 0.1659, 0.2106])\n",
        "print(row_0_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y40kr-uUM19"
      },
      "source": [
        "### Compute All context vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATCLH64zUMp-",
        "outputId": "1d954869-b52f-486d-ac96-aeb1afd52d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7376, 0.7165, 0.3381],\n",
            "        [0.7174, 0.7005, 0.3616],\n",
            "        [0.7221, 0.7060, 0.3419],\n",
            "        [0.7157, 0.6974, 0.3712],\n",
            "        [0.7089, 0.6918, 0.3852],\n",
            "        [0.7223, 0.7036, 0.3584]], grad_fn=<MmBackward0>)\n",
            "Previous 2nd context vector: tensor([0.7174, 0.7005, 0.3616], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ rounded_embeddings\n",
        "print(all_context_vecs)\n",
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VghIaCejQ6JH"
      },
      "source": [
        "## Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1218PmTRnuh"
      },
      "source": [
        "**Weight parameters** are learned coefficients that define the network connections, while **attention weights** are dynamic, context-specific values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObbzoVnCShzc",
        "outputId": "f1bfe37b-2a99-44ec-f911-4f523c8fa19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ],
      "source": [
        "x_2 = rounded_embeddings[1] # second input element\n",
        "d_in = rounded_embeddings.shape[1] #depends on the embeddings dimension the input embedding size, dim=3\n",
        "d_out = 2 # the output embedding size, dim=2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "print(W_query)\n",
        "print(W_key)\n",
        "print(W_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TC8hnCwUce2",
        "outputId": "91fb293a-0ed7-4b59-ad89-9e35c59c9a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3753, 1.1022], grad_fn=<SqueezeBackward4>)\n",
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)\n",
        "\n",
        "# calculate keys and values vector for all inputs\n",
        "keys = rounded_embeddings @ W_key\n",
        "values = rounded_embeddings @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi-rk9d_U3_8",
        "outputId": "4454e850-650a-441e-a634-3778cbe5ae56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0281, grad_fn=<DotBackward0>)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2w0_CCgU030",
        "outputId": "edb910f0-7625-4747-8526-84ab470a43de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1044, 1.0281, 0.6589, 1.0591, 1.2793, 1.0315],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgtjdYYBVrjm"
      },
      "source": [
        "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension,  𝑑𝑘‾‾‾√  (i.e., d_k**0.5):\n",
        "\n",
        "Imagine you have two vectors, and their dot product results in a large value. When this large value is passed through the softmax function, it might dominate the probabilities, making the attention mechanism less sensitive to other relevant parts of the input. Scaling helps to mitigate this issue by preventing any single dot product from becoming overly influential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFekEwTuVWFt",
        "outputId": "63aa5c6c-ad9a-4f44-9024-bc9a57352e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1746, 0.1654, 0.1274, 0.1691, 0.1976, 0.1658],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[1] # dimension of keys\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzEw5VL0XQqE",
        "outputId": "4ef6fddf-a4cb-4479-bd3c-5a012a8a3aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3161, 0.7322], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Dg25JCXccF"
      },
      "source": [
        "### Compact SelfAttention Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPFiRyaKX0Lb"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzWub2DXvao"
      },
      "source": [
        "- We can streamline the implementation above using PyTorch's Linear layers instead of torch random, which are equivalent to a matrix multiplication if we disable the bias units.\n",
        "- In the original Transformer paper, the authors noted that they did not observe significant performance gains from including bias terms in these specific layers. This observation has led to a common practice of omitting bias in similar attention-based architectures.\n",
        "- Another big advantage of using `nn.Linear` over our manual `nn.Parameter(torch.rand(...)` approach is that `nn.Linear` has a preferred weight initialization scheme, which leads to more stable model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Yx74nUgLfNJB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8O5vlV-nXw3p"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2mJv-ETX-g3",
        "outputId": "19b2f8ca-87f9-4a4b-c629-63203cfc2988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0044, 0.2247],\n",
            "        [0.0053, 0.2258],\n",
            "        [0.0059, 0.2266],\n",
            "        [0.0052, 0.2258],\n",
            "        [0.0050, 0.2255],\n",
            "        [0.0051, 0.2256]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(rounded_embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLpyQ2keYaxo"
      },
      "source": [
        "### Hiding futer words with causal attention (one step back)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd9vAnBTYjJp",
        "outputId": "d22d3088-311e-4a14-a419-aa8ff32349f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1561, 0.1632, 0.1462, 0.1800, 0.1864, 0.1680],\n",
            "        [0.1585, 0.1636, 0.1502, 0.1777, 0.1820, 0.1680],\n",
            "        [0.1602, 0.1648, 0.1541, 0.1746, 0.1787, 0.1675],\n",
            "        [0.1584, 0.1632, 0.1498, 0.1784, 0.1821, 0.1682],\n",
            "        [0.1577, 0.1627, 0.1483, 0.1797, 0.1833, 0.1684],\n",
            "        [0.1579, 0.1634, 0.1492, 0.1784, 0.1829, 0.1681]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Reuse data from previous section\n",
        "queries = sa_v2.W_query(rounded_embeddings)\n",
        "keys = sa_v2.W_key(rounded_embeddings)\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKX_phRVdlvv"
      },
      "source": [
        "Applying negative infinity effectively zeros out the probabilities for these future tokens in the subsequent softmax calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmtxjas9cwln",
        "outputId": "e6fbf401-11c7-4201-e5cf-6a02930c25f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0553,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.0413,  0.0866,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.0350,  0.0750, -0.0206,    -inf,    -inf,    -inf],\n",
            "        [ 0.0401,  0.0828, -0.0388,  0.2085,    -inf,    -inf],\n",
            "        [ 0.0420,  0.0860, -0.0450,  0.2268,  0.2548,    -inf],\n",
            "        [ 0.0439,  0.0919, -0.0362,  0.2163,  0.2516,  0.1318]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[-1]\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5mLgJvOdGAr",
        "outputId": "1976afc7-b205-4b2c-ebe6-918944e1ef30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4920, 0.5080, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3344, 0.3440, 0.3215, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2437, 0.2512, 0.2305, 0.2746, 0.0000, 0.0000],\n",
            "        [0.1896, 0.1956, 0.1783, 0.2161, 0.2204, 0.0000],\n",
            "        [0.1579, 0.1634, 0.1492, 0.1784, 0.1829, 0.1681]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-XqXxKcYjbe"
      },
      "source": [
        "### Masking additional attention weights with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGyeNovHe97q",
        "outputId": "2dea6f74-d8a4-4f44-c37e-f38357cfc656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n",
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6431, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5024, 0.0000, 0.5491, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3912, 0.3566, 0.4322, 0.4408, 0.0000],\n",
            "        [0.3159, 0.3268, 0.0000, 0.0000, 0.3659, 0.3361]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6, 6) # create a matrix of ones\n",
        "\n",
        "print(dropout(example))\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9schnK0fRte"
      },
      "source": [
        "### Causal Attention Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "S8GxHW4IfRVU"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eoq6pVwfyzu",
        "outputId": "f9769d6e-ebf5-4f55-e695-87724514689c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "tensor([[[1.0000, 0.9300, 0.1500],\n",
            "         [0.5800, 0.7000, 0.3700],\n",
            "         [0.5700, 0.6500, 0.0000],\n",
            "         [0.7100, 0.5500, 0.5300],\n",
            "         [0.5600, 0.6100, 0.7500],\n",
            "         [0.7600, 0.6800, 0.3600]],\n",
            "\n",
            "        [[1.0000, 0.9300, 0.1500],\n",
            "         [0.5800, 0.7000, 0.3700],\n",
            "         [0.5700, 0.6500, 0.0000],\n",
            "         [0.7100, 0.5500, 0.5300],\n",
            "         [0.5600, 0.6100, 0.7500],\n",
            "         [0.7600, 0.6800, 0.3600]]], grad_fn=<StackBackward0>)\n",
            "2\n",
            "context_shape torch.Size([2, 6, 2])\n",
            "context_vecs: tensor([[[-0.8476, -0.4664],\n",
            "         [-0.7296, -0.3522],\n",
            "         [-0.6553, -0.3511],\n",
            "         [-0.6575, -0.2942],\n",
            "         [-0.6557, -0.2441],\n",
            "         [-0.6602, -0.2456]],\n",
            "\n",
            "        [[-0.8476, -0.4664],\n",
            "         [-0.7296, -0.3522],\n",
            "         [-0.6553, -0.3511],\n",
            "         [-0.6575, -0.2942],\n",
            "         [-0.6557, -0.2441],\n",
            "         [-0.6602, -0.2456]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = torch.stack((rounded_embeddings, rounded_embeddings), dim=0)\n",
        "print(batch.shape) # 2 batches with 6 tokens each, and each token has embedding dimension 3\n",
        "print(batch)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(\"context_shape\", context_vecs.shape)\n",
        "print(\"context_vecs:\", context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UydLlF4rj0hf"
      },
      "source": [
        "## Self Attention multi-head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0f3nXA9ne-Q"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAZxS6r3nWzN",
        "outputId": "0c4bbc0e-2a9f-480d-eed2-a2c9cbe526df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "--------------------\n",
            "torch.Size([2, 6, 2, 1]) vs torch.Size([2, 2, 6, 1])\n",
            "--------------------\n",
            "tensor([[[0.2288, 0.1973],\n",
            "         [0.2357, 0.2711],\n",
            "         [0.2233, 0.3080],\n",
            "         [0.2367, 0.3146],\n",
            "         [0.2476, 0.3221],\n",
            "         [0.2479, 0.3197]],\n",
            "\n",
            "        [[0.2288, 0.1973],\n",
            "         [0.2357, 0.2711],\n",
            "         [0.2233, 0.3080],\n",
            "         [0.2367, 0.3146],\n",
            "         [0.2476, 0.3221],\n",
            "         [0.2479, 0.3197]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "# Multiple heads to extract differente type of information, every head is using different initialized weights\n",
        "# cada head da como resultado context vectors de cierte dimension que al final son concatenados\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "                             \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # b is for batches\n",
        "        print(x.shape)\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        print('--------------------')\n",
        "        print(f'{keys.shape} vs {keys.transpose(1,2).shape}')\n",
        "        print('--------------------')\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "123g1w8Vz_0T",
        "KiqznPKxE2-J",
        "kQyY8us8kjnU",
        "sdqVKwHw9WOr",
        "e5oaJW523vt2",
        "b0nYzc5uBMkF",
        "cXjmjDeM6z-5",
        "t-qc5coF7P9c",
        "scsHf8OgJiSX",
        "N3bdp26B93oU",
        "JGLUALUt968u",
        "SC3hdDBCQyxj",
        "4gYidQzDQAI5",
        "UryRjq28FZ39"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}