{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elyager/LLMs-from-scratch/blob/main/LLM_from_scratch_personal_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123g1w8Vz_0T"
      },
      "source": [
        "### Requiriments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCSj5qDn9bP-",
        "outputId": "14b3909f-4054-4182-9fbe-92b85e01d175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqznPKxE2-J"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQd3ILu5Czb"
      },
      "source": [
        "- **vocab_size:** es el tamaño del vocabulario en tokens únicos disponibles+extensiones. En nuestro caso dado por el tokenizador creado con BPE.\n",
        "- **output_dim:** el número de dimensiones de cada token. Las dimensiones describen a una palabra o concepto. Más dimensiones capturan más detalles.\n",
        "- **max_length:** es la máxima logitud de tokens por secuencia.\n",
        "- **batch_size:** es cuantas secuencias (muestras de texto) tiene cada batch. Larger batch sizes can speed up training but might require more memory.\n",
        "- **stride:** el tamaño de la zancada en tokens, cuantos tokens salta para la siguiente secuencia, esto determina que tanto se empalma una secuencia con otra. Ensure the model sees the context of each token multiple times during training, helping it learn better relationships between words.\n",
        "- **shuffle:** determina si le da un orden aleatorio a las secuencias para que sea random en cada epoc. Prevents the model from learning patterns based on the order of data presentation. It helps generalize learning and avoid overfitting to a specific data order.\n",
        "-**drop_last:** indica si se debe descartar el último batch cuando no cumple con el número de muestras establecido en max_lenght. Si se tiene un set de datos pequeño es mejor no hacer drop.\n",
        "-**num_workers:** es el número de procesos, a mayor número más rápidez.\n",
        "-**attention score:** determina qué tan \"similar\" es una pababra con la otra a través de dot product que es un tipo de similarity function. Mayor atention score mayor similitud entre los números.\n",
        "--**attention weight:** es la versión normalizada a través de softmax de los attention scores.\n",
        "-**context vector:** es un embedding vector pero que tiene todo el contexto del resto de input vectors. Se obtiene sumando todos los attention weights del inpute secuence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZbR6dXofBXJx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN__3TwCBXiO"
      },
      "source": [
        "# Chapter 1 & 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyY8us8kjnU",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load the text for training (our corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLL4xfSX9C0p",
        "outputId": "3225e8c0-9a61-4ea5-8a8c-8de37942469c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqVKwHw9WOr",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### BytePair Encoding (BPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmSakIi9grv",
        "outputId": "4b8eabe8-bdc1-4385-9e8d-3c57ce656d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoAAtvXt9r53",
        "outputId": "b78193a8-41d6-4c46-8ba2-00de47f191a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I |-| H |-|AD |-| always |-| thought |-| Jack |-| G |-|is |-|burn |-| rather\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n",
            "\n",
            " Total of tokens: 5146\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
        "enc_text.append(tokenizer.eot_token)\n",
        "\n",
        "# First 10 tokens from raw_text\n",
        "first_10_token_ids = enc_text[:10]\n",
        "decoded_tokens = [tokenizer.decode([token_id]) for token_id in first_10_token_ids]\n",
        "delimited_tokens = ' |-|'.join(decoded_tokens)\n",
        "print(delimited_tokens)\n",
        "print(enc_text[:10])\n",
        "print(f'\\n Total of tokens: {len(enc_text)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5oaJW523vt2",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Dataset loader (creating tokenIDs for inputs and targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o6B2KxAkAbLC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        token_ids.append(tokenizer.eot_token)\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# dataset = GPTDatasetV1(raw_text, tokenizer, max_length=4, stride=1)\n",
        "# print(dataset.input_ids)\n",
        "# print(dataset.target_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KXlE1wG6AsOk"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0nYzc5uBMkF"
      },
      "source": [
        "### Use DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gIJnkOxAvfj",
        "outputId": "e4f56cc1-bea5-4600-bc33-2f97105f67de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "# for batch in dataloader:\n",
        "#     input, target = batch\n",
        "#     print(input, target)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch) # input and target\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch) # input and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Oyu9uJEay2",
        "outputId": "a615a1c8-0417-4f3f-9c11-de05a49275f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "Inputs:\n",
            " I HAD always\n",
            "\n",
            "Targets:\n",
            "  HAD always thought\n",
            "Inputs:\n",
            " tensor([[  287,   262,  6001,   286],\n",
            "        [  465, 13476,    11,   339],\n",
            "        [  550,  5710,   465, 12036],\n",
            "        [   11,  6405,   257,  5527],\n",
            "        [27075,    11,   290,  4920],\n",
            "        [ 2241,   287,   257,  4489],\n",
            "        [   64,   319,   262, 34686],\n",
            "        [41976,    13,   357, 10915]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  262,  6001,   286,   465],\n",
            "        [13476,    11,   339,   550],\n",
            "        [ 5710,   465, 12036,    11],\n",
            "        [ 6405,   257,  5527, 27075],\n",
            "        [   11,   290,  4920,  2241],\n",
            "        [  287,   257,  4489,    64],\n",
            "        [  319,   262, 34686, 41976],\n",
            "        [   13,   357, 10915,   314]])\n",
            "Inputs:\n",
            "  in the height of\n",
            "\n",
            "Targets:\n",
            "  the height of his\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "# First batch\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
        "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))\n",
        "\n",
        "# Second batch\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "print(\"Inputs:\\n\", tokenizer.decode(inputs[0].tolist()))\n",
        "print(\"\\nTargets:\\n\", tokenizer.decode(targets[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXjmjDeM6z-5"
      },
      "source": [
        "### Create our token embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax4UNR13E4wB",
        "outputId": "2a7f6ef1-bcbd-406c-fa6e-3402e51c1494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257, 3])\n",
            "Parameter containing:\n",
            "tensor([[ 1.0499,  1.5613,  1.5356],\n",
            "        [ 0.0054,  0.4545, -0.2530],\n",
            "        [ 0.6044, -0.0970,  1.2713],\n",
            "        ...,\n",
            "        [ 0.8461,  0.2061, -0.7791],\n",
            "        [ 0.1027,  0.4897,  0.3037],\n",
            "        [ 1.8263,  1.7426,  0.0666]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 3 # 256 is a more common starting point\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(token_embedding_layer.weight.shape)\n",
        "print(token_embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaPfzy6d631b"
      },
      "source": [
        "#### Load our dataset to get the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuqLpfanGGBp",
        "outputId": "94aee915-e59b-47f4-c2d5-b413392665b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Token IDs:\\n\",  inputs) # we take the first batch and  ignore the targets for now\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-qc5coF7P9c"
      },
      "source": [
        "### Create token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ziD4fWmIUMy",
        "outputId": "c846840f-f687-4985-e024-75d8de9c03e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 3])\n",
            "tensor([[[-1.2239,  0.9454, -1.0618],\n",
            "         [ 1.6218, -0.3814,  0.6126],\n",
            "         [ 0.1450,  1.1302, -0.6425],\n",
            "         [-0.5892, -1.5937,  0.0803]],\n",
            "\n",
            "        [[-0.8558, -0.9859,  0.2085],\n",
            "         [ 0.3994, -0.3635,  0.2396],\n",
            "         [ 0.4786,  0.4042, -1.1887],\n",
            "         [ 0.6692,  0.0512,  1.8413]],\n",
            "\n",
            "        [[-0.2117, -2.3216, -1.0187],\n",
            "         [ 0.1332,  1.5330,  0.2707],\n",
            "         [-0.2937, -0.0921, -1.4307],\n",
            "         [ 0.3152, -0.6272, -1.1681]],\n",
            "\n",
            "        [[-0.3351, -1.3230, -1.5735],\n",
            "         [-0.9770, -0.3570,  0.9478],\n",
            "         [ 0.3268, -1.7190,  0.0553],\n",
            "         [-0.2937, -0.0921, -1.4307]],\n",
            "\n",
            "        [[ 0.8881,  0.8748, -0.4730],\n",
            "         [ 1.9018,  0.9325,  0.1734],\n",
            "         [ 0.4939,  0.5552, -0.2040],\n",
            "         [-0.9770, -0.3570,  0.9478]],\n",
            "\n",
            "        [[-1.1258,  1.8030, -0.1891],\n",
            "         [ 1.3732,  0.1376, -0.0471],\n",
            "         [-0.9402,  0.0433,  1.1018],\n",
            "         [-0.3877,  0.1870,  0.2711]],\n",
            "\n",
            "        [[ 0.0903,  2.2378,  2.1044],\n",
            "         [-0.6160,  0.5208,  0.0074],\n",
            "         [ 0.0999, -0.5945,  0.5404],\n",
            "         [ 1.7530,  0.3059,  0.8458]],\n",
            "\n",
            "        [[ 0.0999, -0.5945,  0.5404],\n",
            "         [-0.5307,  0.5056,  1.4472],\n",
            "         [-1.9575,  1.3100,  0.0107],\n",
            "         [ 0.2660, -0.4286,  1.5288]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "# each token now has the assigned number of dimentions instead of being a single token ID\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(token_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scsHf8OgJiSX"
      },
      "source": [
        "### Create absolute positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "549fxfhkJh_V",
        "outputId": "44be8461-474e-4534-af8c-b2122892cec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n",
            "tensor([[-0.2958,  0.0469, -0.1967],\n",
            "        [ 0.7208,  0.6753, -0.3788],\n",
            "        [-0.5114,  0.2414,  1.1345],\n",
            "        [-0.6289,  0.0640,  0.8413]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "\n",
        "# # [0, 1, 2, 3] \"column\" position is the position of each word on each sequence of 4 context_length\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "# # uncomment & execute the following line to see how the embeddings look like\n",
        "print(pos_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3bdp26B93oU"
      },
      "source": [
        "### Create input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F00jaZjOKPpw",
        "outputId": "e3197893-50b1-429f-caf5-42fafa4e8874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 3]) + torch.Size([4, 3]) = torch.Size([8, 4, 3])\n",
            "tensor([[[-1.5197,  0.9923, -1.2585],\n",
            "         [ 2.3426,  0.2939,  0.2338],\n",
            "         [-0.3665,  1.3716,  0.4920],\n",
            "         [-1.2181, -1.5297,  0.9216]],\n",
            "\n",
            "        [[-1.1515, -0.9390,  0.0118],\n",
            "         [ 1.1202,  0.3118, -0.1392],\n",
            "         [-0.0329,  0.6456, -0.0542],\n",
            "         [ 0.0402,  0.1152,  2.6826]],\n",
            "\n",
            "        [[-0.5075, -2.2746, -1.2154],\n",
            "         [ 0.8540,  2.2083, -0.1081],\n",
            "         [-0.8052,  0.1493, -0.2962],\n",
            "         [-0.3137, -0.5632, -0.3269]],\n",
            "\n",
            "        [[-0.6309, -1.2760, -1.7702],\n",
            "         [-0.2562,  0.3183,  0.5690],\n",
            "         [-0.1846, -1.4776,  1.1898],\n",
            "         [-0.9227, -0.0280, -0.5894]],\n",
            "\n",
            "        [[ 0.5923,  0.9218, -0.6697],\n",
            "         [ 2.6226,  1.6078, -0.2054],\n",
            "         [-0.0176,  0.7966,  0.9305],\n",
            "         [-1.6060, -0.2930,  1.7891]],\n",
            "\n",
            "        [[-1.4216,  1.8499, -0.3858],\n",
            "         [ 2.0940,  0.8129, -0.4260],\n",
            "         [-1.4516,  0.2847,  2.2363],\n",
            "         [-1.0166,  0.2510,  1.1124]],\n",
            "\n",
            "        [[-0.2055,  2.2848,  1.9077],\n",
            "         [ 0.1048,  1.1961, -0.3714],\n",
            "         [-0.4115, -0.3531,  1.6749],\n",
            "         [ 1.1240,  0.3699,  1.6870]],\n",
            "\n",
            "        [[-0.1959, -0.5476,  0.3437],\n",
            "         [ 0.1902,  1.1809,  1.0684],\n",
            "         [-2.4689,  1.5514,  1.1452],\n",
            "         [-0.3630, -0.3646,  2.3701]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(input_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGLUALUt968u"
      },
      "source": [
        "### What about more dimmension?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SRFPsblZ-CfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913c5807-c1dd-4cb4-8333-0998bd4da81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "tensor([[[ 0.4284, -0.0444, -1.5124,  ..., -1.1995,  1.2147,  0.1603],\n",
            "         [ 0.9657,  0.6022,  1.4287,  ..., -1.0535, -1.4809,  0.7132],\n",
            "         [-0.4677, -1.9291, -0.1119,  ..., -0.5154,  0.3336,  0.1206],\n",
            "         [-0.1478, -0.9211,  0.2747,  ...,  1.8229,  0.4754, -0.9500]],\n",
            "\n",
            "        [[-2.0476, -1.9832, -1.4409,  ..., -0.6543, -0.3288, -0.4750],\n",
            "         [ 0.3891,  0.2687,  0.0831,  ...,  0.3362,  1.1893,  0.4639],\n",
            "         [-0.3365,  0.6894,  0.1393,  ..., -0.9712, -1.0528,  2.0448],\n",
            "         [ 0.0072, -0.4715, -0.5916,  ...,  0.5685,  0.0201, -0.8140]],\n",
            "\n",
            "        [[ 0.8937, -1.0620, -1.6252,  ..., -0.1977, -1.7489,  0.0119],\n",
            "         [-1.1047, -0.3365,  1.0697,  ...,  1.5743, -2.3146,  1.9848],\n",
            "         [-0.8749, -1.7211, -1.0064,  ...,  0.1872, -0.8916,  0.9201],\n",
            "         [-0.2906, -1.0940, -0.5154,  ..., -0.0558, -1.2010,  1.0564]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.6405,  0.6699,  1.3605,  ...,  1.1404,  0.4990,  1.4791],\n",
            "         [ 1.2230, -1.5099,  1.2634,  ...,  1.2759,  0.1792,  0.1306],\n",
            "         [ 1.5060,  0.1011, -0.7881,  ..., -0.5343, -0.1676, -0.8398],\n",
            "         [-0.6482, -0.0688, -1.0381,  ...,  1.6798, -0.2439,  1.9247]],\n",
            "\n",
            "        [[ 1.1953, -1.5037, -0.0970,  ...,  1.4523,  0.5751, -0.9565],\n",
            "         [ 0.2795,  0.8001,  2.7197,  ..., -0.7298, -1.0214, -0.6170],\n",
            "         [-0.2534,  3.0511, -0.4479,  ..., -2.4842, -1.6493,  0.2953],\n",
            "         [ 0.1162, -0.0995,  0.2258,  ..., -0.8851,  3.4623,  0.8882]],\n",
            "\n",
            "        [[-0.2534,  3.0511, -0.4479,  ..., -2.4842, -1.6493,  0.2953],\n",
            "         [-1.6919, -1.5010, -0.5925,  ...,  3.0840,  0.4809, -0.0420],\n",
            "         [ 0.8871,  0.8799, -2.0292,  ..., -0.2751,  0.2091,  0.3121],\n",
            "         [-0.5842, -0.1449,  0.7288,  ..., -1.6238,  1.2948, -0.1485]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([4, 256])\n",
            "tensor([[ 0.8528, -0.2721,  0.2335,  ...,  0.1843, -0.6053, -0.8407],\n",
            "        [ 0.4576, -1.6117,  0.4703,  ..., -0.5022, -0.3112,  0.2395],\n",
            "        [-2.0840,  2.0597,  0.6903,  ..., -0.2708,  1.4528,  0.9685],\n",
            "        [ 0.9481, -0.8510,  0.3127,  ...,  1.5657, -0.1655,  0.2697]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([8, 4, 256]) + torch.Size([4, 256]) = torch.Size([8, 4, 256])\n",
            "tensor([[[ 1.2812, -0.3165, -1.2789,  ..., -1.0152,  0.6093, -0.6804],\n",
            "         [ 1.4233, -1.0095,  1.8990,  ..., -1.5557, -1.7921,  0.9527],\n",
            "         [-2.5517,  0.1306,  0.5784,  ..., -0.7861,  1.7864,  1.0891],\n",
            "         [ 0.8003, -1.7721,  0.5873,  ...,  3.3886,  0.3099, -0.6804]],\n",
            "\n",
            "        [[-1.1948, -2.2553, -1.2074,  ..., -0.4700, -0.9342, -1.3158],\n",
            "         [ 0.8467, -1.3430,  0.5534,  ..., -0.1660,  0.8781,  0.7034],\n",
            "         [-2.4206,  2.7490,  0.8296,  ..., -1.2420,  0.3999,  3.0133],\n",
            "         [ 0.9553, -1.3225, -0.2789,  ...,  2.1341, -0.1454, -0.5444]],\n",
            "\n",
            "        [[ 1.7465, -1.3342, -1.3917,  ..., -0.0134, -2.3542, -0.8288],\n",
            "         [-0.6470, -1.9482,  1.5400,  ...,  1.0721, -2.6258,  2.2243],\n",
            "         [-2.9590,  0.3386, -0.3161,  ..., -0.0835,  0.5611,  1.8885],\n",
            "         [ 0.6575, -1.9450, -0.2028,  ...,  1.5098, -1.3665,  1.3261]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.4933,  0.3978,  1.5939,  ...,  1.3247, -0.1063,  0.6384],\n",
            "         [ 1.6806, -3.1216,  1.7337,  ...,  0.7737, -0.1320,  0.3701],\n",
            "         [-0.5780,  2.1608, -0.0979,  ..., -0.8050,  1.2851,  0.1286],\n",
            "         [ 0.2999, -0.9198, -0.7254,  ...,  3.2454, -0.4094,  2.1944]],\n",
            "\n",
            "        [[ 2.0480, -1.7759,  0.1365,  ...,  1.6366, -0.0303, -1.7972],\n",
            "         [ 0.7372, -0.8115,  3.1900,  ..., -1.2320, -1.3325, -0.3775],\n",
            "         [-2.3374,  5.1108,  0.2424,  ..., -2.7549, -0.1965,  1.2638],\n",
            "         [ 1.0643, -0.9505,  0.5385,  ...,  0.6805,  3.2968,  1.1578]],\n",
            "\n",
            "        [[ 0.5993,  2.7790, -0.2144,  ..., -2.2999, -2.2546, -0.5454],\n",
            "         [-1.2342, -3.1127, -0.1222,  ...,  2.5818,  0.1697,  0.1975],\n",
            "         [-1.1969,  2.9395, -1.3389,  ..., -0.5459,  1.6619,  1.2806],\n",
            "         [ 0.3639, -0.9959,  1.0415,  ..., -0.0581,  1.1293,  0.1212]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def more_more_more_dimensions(dim):\n",
        "  vocab_size = 50257\n",
        "  output_dim = dim\n",
        "\n",
        "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "  token_embeddings = token_embedding_layer(inputs)\n",
        "  print(token_embeddings.shape)\n",
        "  print(token_embeddings)\n",
        "\n",
        "  pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "  pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "  print(pos_embeddings.shape)\n",
        "  print(pos_embeddings)\n",
        "\n",
        "\n",
        "  input_embeddings = token_embeddings + pos_embeddings\n",
        "  print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "  print(input_embeddings)\n",
        "\n",
        "more_more_more_dimensions(256)  #256 is a more common starting point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbTcK0ieCyHO"
      },
      "source": [
        "# Chapter 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC3hdDBCQyxj"
      },
      "source": [
        "## Preparing data to work with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL4kUkq1FNVy"
      },
      "source": [
        "### Get input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DUVjip9OAjSn"
      },
      "outputs": [],
      "source": [
        "def get_input_embeddings(my_raw_text):\n",
        "  dataloader = create_dataloader_v1(my_raw_text, batch_size=1, max_length=6, stride=6, shuffle=False)\n",
        "\n",
        "  data_iter = iter(dataloader)\n",
        "  # First and only batch\n",
        "  inputs, targets = next(data_iter)\n",
        "  # print(inputs)\n",
        "  # print(targets)  # ignore the targets\n",
        "\n",
        "  vocab_size = 50257\n",
        "  output_dim = 3\n",
        "\n",
        "  token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "  token_embeddings = token_embedding_layer(inputs)\n",
        "  context_length = 6\n",
        "  pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "  pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "  input_embeddings = token_embeddings + pos_embeddings\n",
        "  # print(f'{token_embeddings.shape} + {pos_embeddings.shape} = {input_embeddings.shape}')\n",
        "  # print(input_embeddings)\n",
        "  return input_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQzrouXuGICO",
        "outputId": "7f6be946-f4a2-4279-f01c-f8ab76bdf378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0610, -0.3015, -1.7378],\n",
            "        [ 0.8547, -0.7702, -0.6719],\n",
            "        [ 2.6666,  2.5809, -0.0577],\n",
            "        [-0.2783, -0.9558, -0.0836],\n",
            "        [-0.3893, -0.3262, -2.2585],\n",
            "        [ 0.2071,  2.5861,  1.3067]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "my_raw_text = \"Your journey starts with one step\"\n",
        "small_input_embeddings = get_input_embeddings(my_raw_text)[0]\n",
        "print(small_input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FkdLF-xTIFdB"
      },
      "outputs": [],
      "source": [
        "# tensor([\n",
        "#     [ 0.4113,  1.3397, -1.2234], Your    (x^1)\n",
        "#     [-1.8881, -0.0679, -1.1267], journey (x^2)\n",
        "#     [-0.2323, -2.2089, -1.6685], starts  (x^3)\n",
        "#     [ 0.5615,  1.2698,  2.5768], with    (x^4)\n",
        "#     [-0.9290, -0.0227,  0.6467], one     (x^5)\n",
        "#     [ 0.5691, -2.0627, -3.2411]  step    (x^6)\n",
        "# ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykfq4LulMWSG"
      },
      "source": [
        "### Forcing values to fit between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf5_7By8Ma-V",
        "outputId": "58eed89c-e7bc-466b-d69f-55467a23ed49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2400, 0.4000, 0.1100],\n",
            "        [0.6300, 0.3000, 0.3200],\n",
            "        [1.0000, 0.9800, 0.4500],\n",
            "        [0.4000, 0.2600, 0.4400],\n",
            "        [0.3800, 0.3900, 0.0000],\n",
            "        [0.5000, 0.9800, 0.7200]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import decimal\n",
        "min_val = small_input_embeddings.min()\n",
        "max_val = small_input_embeddings.max()\n",
        "scaled_embeddings = (small_input_embeddings - min_val) / (max_val - min_val)\n",
        "rounded_embeddings = torch.round(scaled_embeddings * 100) / 100\n",
        "print(rounded_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gYidQzDQAI5"
      },
      "source": [
        "## Simple self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UryRjq28FZ39"
      },
      "source": [
        "### Step 1 - Compute unormalized attention scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6wJC7RJU3c",
        "outputId": "7284d336-171f-4b3c-fed7-ff5525bdbf73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dot product of tensor([0.2400, 0.4000, 0.1100], grad_fn=<UnbindBackward0>) against journey tensor([0.6300, 0.3000, 0.3200], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.6300, 0.3000, 0.3200], grad_fn=<UnbindBackward0>) against journey tensor([0.6300, 0.3000, 0.3200], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([1.0000, 0.9800, 0.4500], grad_fn=<UnbindBackward0>) against journey tensor([0.6300, 0.3000, 0.3200], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.4000, 0.2600, 0.4400], grad_fn=<UnbindBackward0>) against journey tensor([0.6300, 0.3000, 0.3200], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.3800, 0.3900, 0.0000], grad_fn=<UnbindBackward0>) against journey tensor([0.6300, 0.3000, 0.3200], grad_fn=<SelectBackward0>)\n",
            "dot product of tensor([0.5000, 0.9800, 0.7200], grad_fn=<UnbindBackward0>) against journey tensor([0.6300, 0.3000, 0.3200], grad_fn=<SelectBackward0>)\n",
            "tensor([0.3064, 0.5893, 1.0680, 0.4708, 0.3564, 0.8394], grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "query = rounded_embeddings[1]  # 2nd input token is the query)\n",
        "# just allocate a tensor in memory with 6 spaces\n",
        "attn_scores_2 = torch.empty(rounded_embeddings.shape[0])\n",
        "\n",
        "#fill the tensor with the dot products which multiply and sum\n",
        "for i, x_i in enumerate(rounded_embeddings):\n",
        "    print(f'dot product of {x_i} against journey {query}')\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMsw6j8WJsFU"
      },
      "source": [
        "### Step 2 - Normalize the attenton scores to sum up to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PF1wtEJJrnj",
        "outputId": "6480a976-60f7-4c04-f58d-597dec11a07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1190, 0.1579, 0.2549, 0.1403, 0.1251, 0.2028],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Sum: tensor(1.0000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MB0l0Y-a090",
        "outputId": "62177808-37cd-4fc9-c169-20e5a453babd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "torch.Size([2, 3, 4])\n",
            "4\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Fooling around with dimensions on vectors\n",
        "my_tensor = torch.ones(2,3,4) # I always start with the last dimension which is [-1]\n",
        "print(my_tensor)\n",
        "print(my_tensor.shape)\n",
        "print(my_tensor.shape[-1]) # last dimension\n",
        "print(my_tensor.shape[0]) # first dimenson\n",
        "print(my_tensor.shape[1])\n",
        "print(my_tensor.shape[2]) # same as [-1]\n",
        "# print(my_tensor.shape[3]) # IndexError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIYCzERyLwBQ",
        "outputId": "85371a79-2775-4486-bdbb-a42af7ef6f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1190, 0.1579, 0.2549, 0.1403, 0.1251, 0.2028],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Sum: tensor(1.0000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# using pytorch softmax fucntion\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum()) #100%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsNQPLyNQ9DN"
      },
      "source": [
        "### Step 3 - Compute the context vector $z^{(2)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tba1xAyQ5G2",
        "outputId": "1a40bf7f-e01e-4a8c-9625-a0c51796fd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11900985240936279 * tensor([0.2400, 0.4000, 0.1100], grad_fn=<UnbindBackward0>)\n",
            "0.15792278945446014 * tensor([0.6300, 0.3000, 0.3200], grad_fn=<UnbindBackward0>)\n",
            "0.25488343834877014 * tensor([1.0000, 0.9800, 0.4500], grad_fn=<UnbindBackward0>)\n",
            "0.14027521014213562 * tensor([0.4000, 0.2600, 0.4400], grad_fn=<UnbindBackward0>)\n",
            "0.1251116245985031 * tensor([0.3800, 0.3900, 0.0000], grad_fn=<UnbindBackward0>)\n",
            "0.20279717445373535 * tensor([0.5000, 0.9800, 0.7200], grad_fn=<UnbindBackward0>)\n",
            "tensor(1.0000, grad_fn=<AddBackward0>)\n",
            "tensor([0.5880, 0.6288, 0.3861], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "query = rounded_embeddings[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "attn_weights_sum = 0\n",
        "for i,x_i in enumerate(rounded_embeddings):\n",
        "    print(f'{attn_weights_2[i]} * {x_i}')\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "    attn_weights_sum += attn_weights_2[i]\n",
        "\n",
        "print(attn_weights_sum)\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtDPseV2Tgyp"
      },
      "source": [
        "### Get All attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDMWyFwTgmA",
        "outputId": "11e403e8-cb85-4766-c166-bdda8e4ebafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2297, 0.3064, 0.6815, 0.2484, 0.2472, 0.5912],\n",
            "        [0.3064, 0.5893, 1.0680, 0.4708, 0.3564, 0.8394],\n",
            "        [0.6815, 1.0680, 2.1629, 0.8528, 0.7622, 1.7844],\n",
            "        [0.2484, 0.4708, 0.8528, 0.4212, 0.2534, 0.7716],\n",
            "        [0.2472, 0.3564, 0.7622, 0.2534, 0.2965, 0.5722],\n",
            "        [0.5912, 0.8394, 1.7844, 0.7716, 0.5722, 1.7288]],\n",
            "       grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(rounded_embeddings):\n",
        "    for j, x_j in enumerate(rounded_embeddings):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvNIm4n3TsBT"
      },
      "source": [
        "We can achive the same but more efficiently via matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaHP2VdZCouE",
        "outputId": "c120fc48-6f0f-49c1-85df-424de5f24e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n",
            "torch.Size([3, 6])\n",
            "-------------------\n",
            "tensor([[0.2400, 0.4000, 0.1100],\n",
            "        [0.6300, 0.3000, 0.3200],\n",
            "        [1.0000, 0.9800, 0.4500],\n",
            "        [0.4000, 0.2600, 0.4400],\n",
            "        [0.3800, 0.3900, 0.0000],\n",
            "        [0.5000, 0.9800, 0.7200]], grad_fn=<DivBackward0>)\n",
            "-------------------\n",
            "tensor([[0.2400, 0.6300, 1.0000, 0.4000, 0.3800, 0.5000],\n",
            "        [0.4000, 0.3000, 0.9800, 0.2600, 0.3900, 0.9800],\n",
            "        [0.1100, 0.3200, 0.4500, 0.4400, 0.0000, 0.7200]],\n",
            "       grad_fn=<PermuteBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Using matrix transpose (remember is row * column)\n",
        "print(rounded_embeddings.shape)\n",
        "print(rounded_embeddings.T.shape)\n",
        "print('-------------------')\n",
        "print(rounded_embeddings)\n",
        "print('-------------------')\n",
        "print(rounded_embeddings.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MeXFpyjTwRW",
        "outputId": "ba051ed2-1c2d-4c85-e980-56f35e8142eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2297, 0.3064, 0.6815, 0.2484, 0.2472, 0.5912],\n",
            "        [0.3064, 0.5893, 1.0680, 0.4708, 0.3564, 0.8394],\n",
            "        [0.6815, 1.0680, 2.1629, 0.8528, 0.7622, 1.7844],\n",
            "        [0.2484, 0.4708, 0.8528, 0.4212, 0.2534, 0.7716],\n",
            "        [0.2472, 0.3564, 0.7622, 0.2534, 0.2965, 0.5722],\n",
            "        [0.5912, 0.8394, 1.7844, 0.7716, 0.5722, 1.7288]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = rounded_embeddings @ rounded_embeddings.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbaMTFAmT4jD"
      },
      "source": [
        "Apply softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNxM31OqT3r2",
        "outputId": "7de51753-ae8c-41dc-b38f-cce7ef4bf03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1404, 0.1516, 0.2206, 0.1430, 0.1429, 0.2015],\n",
            "        [0.1190, 0.1579, 0.2549, 0.1403, 0.1251, 0.2028],\n",
            "        [0.0823, 0.1211, 0.3619, 0.0976, 0.0892, 0.2479],\n",
            "        [0.1256, 0.1569, 0.2299, 0.1493, 0.1263, 0.2120],\n",
            "        [0.1383, 0.1543, 0.2315, 0.1392, 0.1453, 0.1914],\n",
            "        [0.0919, 0.1178, 0.3032, 0.1101, 0.0902, 0.2868]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# dim=-1 is to apply softmax to the last dimenison, in this case rows\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPREekd0T-_j",
        "outputId": "79b15e57-c3c5-4490-d5ef-ee1bd4c8b4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "       grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "row_0_sum = sum([0.1403, 0.1365, 0.1915, 0.1552, 0.1659, 0.2106])\n",
        "print(row_0_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y40kr-uUM19"
      },
      "source": [
        "### Compute All context vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATCLH64zUMp-",
        "outputId": "276e0a5c-6486-429a-f629-cf18af133986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5620, 0.6082, 0.3713],\n",
            "        [0.5880, 0.6288, 0.3861],\n",
            "        [0.6548, 0.7270, 0.4321],\n",
            "        [0.5726, 0.6184, 0.3858],\n",
            "        [0.5685, 0.6089, 0.3678],\n",
            "        [0.6212, 0.7141, 0.4392]], grad_fn=<MmBackward0>)\n",
            "Previous 2nd context vector: tensor([0.5880, 0.6288, 0.3861], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ rounded_embeddings\n",
        "print(all_context_vecs)\n",
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VghIaCejQ6JH"
      },
      "source": [
        "## Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1218PmTRnuh"
      },
      "source": [
        "**Weight parameters** are learned coefficients that define the network connections, while **attention weights** are dynamic, context-specific values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObbzoVnCShzc",
        "outputId": "e03521af-a906-417a-e344-46997c9946db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ],
      "source": [
        "x_2 = rounded_embeddings[1] # second input element\n",
        "d_in = rounded_embeddings.shape[1] #depends on the embeddings dimension the input embedding size, dim=3\n",
        "d_out = 2 # the output embedding size, dim=2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "print(W_query)\n",
        "print(W_key)\n",
        "print(W_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TC8hnCwUce2",
        "outputId": "fb7cba03-94a0-43c9-8c22-f6f56a38be66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2857, 0.8093], grad_fn=<SqueezeBackward4>)\n",
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)\n",
        "\n",
        "# calculate keys and values vector for all inputs\n",
        "keys = rounded_embeddings @ W_key\n",
        "values = rounded_embeddings @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi-rk9d_U3_8",
        "outputId": "fadacf12-f6f9-4121-c26e-634ead9afa6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4757, grad_fn=<DotBackward0>)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2w0_CCgU030",
        "outputId": "37f4900d-c195-4735-c9e2-5dcd06f4f159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3565, 0.4757, 1.0404, 0.4996, 0.2961, 1.1539],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgtjdYYBVrjm"
      },
      "source": [
        "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension,  𝑑𝑘‾‾‾√  (i.e., d_k**0.5):\n",
        "\n",
        "Imagine you have two vectors, and their dot product results in a large value. When this large value is passed through the softmax function, it might dominate the probabilities, making the attention mechanism less sensitive to other relevant parts of the input. Scaling helps to mitigate this issue by preventing any single dot product from becoming overly influential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFekEwTuVWFt",
        "outputId": "8c92693f-8a0b-4505-c5bf-fd8b15040f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1328, 0.1444, 0.2153, 0.1469, 0.1272, 0.2333],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[1] # dimension of keys\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzEw5VL0XQqE",
        "outputId": "f7dac0f9-276e-41b8-aaf6-d5fad53243fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2862, 0.6841], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Dg25JCXccF"
      },
      "source": [
        "### Compact SelfAttention Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPFiRyaKX0Lb"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzWub2DXvao"
      },
      "source": [
        "- We can streamline the implementation above using PyTorch's Linear layers instead of torch random, which are equivalent to a matrix multiplication if we disable the bias units\n",
        "- Another big advantage of using `nn.Linear` over our manual `nn.Parameter(torch.rand(...)` approach is that `nn.Linear` has a preferred weight initialization scheme, which leads to more stable model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Yx74nUgLfNJB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8O5vlV-nXw3p"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2mJv-ETX-g3",
        "outputId": "555832a5-b89a-4076-dafc-97bd80863095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0125,  0.1567],\n",
            "        [-0.0123,  0.1572],\n",
            "        [-0.0129,  0.1581],\n",
            "        [-0.0121,  0.1572],\n",
            "        [-0.0126,  0.1566],\n",
            "        [-0.0125,  0.1582]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(rounded_embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLpyQ2keYaxo"
      },
      "source": [
        "### Hiding futer words with causal attention (one step back)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd9vAnBTYjJp",
        "outputId": "86bb3994-c7ec-4050-e5d4-f9369ea23bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1609, 0.1703, 0.1687, 0.1708, 0.1603, 0.1689],\n",
            "        [0.1583, 0.1731, 0.1694, 0.1732, 0.1579, 0.1681],\n",
            "        [0.1494, 0.1786, 0.1725, 0.1796, 0.1481, 0.1718],\n",
            "        [0.1591, 0.1731, 0.1691, 0.1728, 0.1590, 0.1670],\n",
            "        [0.1610, 0.1699, 0.1688, 0.1706, 0.1603, 0.1695],\n",
            "        [0.1503, 0.1790, 0.1720, 0.1794, 0.1494, 0.1699]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Reuse data from previous section\n",
        "queries = sa_v2.W_query(rounded_embeddings)\n",
        "keys = sa_v2.W_key(rounded_embeddings)\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKX_phRVdlvv"
      },
      "source": [
        "Applying negative infinity effectively zeros out the probabilities for these future tokens in the subsequent softmax calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmtxjas9cwln",
        "outputId": "a42ea72c-a23d-4b25-c51c-337616d7e48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.5175e-04,        -inf,        -inf,        -inf,        -inf,\n",
            "                -inf],\n",
            "        [-6.5602e-03,  1.2047e-01,        -inf,        -inf,        -inf,\n",
            "                -inf],\n",
            "        [-5.4193e-03,  2.4702e-01,  1.9771e-01,        -inf,        -inf,\n",
            "                -inf],\n",
            "        [-9.1383e-03,  1.1044e-01,  7.7196e-02,  1.0829e-01,        -inf,\n",
            "                -inf],\n",
            "        [ 2.0232e-03,  7.7698e-02,  6.8403e-02,  8.3507e-02, -4.7449e-03,\n",
            "                -inf],\n",
            "        [-1.0605e-02,  2.3689e-01,  1.8059e-01,  2.4008e-01, -1.8712e-02,\n",
            "          1.6290e-01]], grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[-1]\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5mLgJvOdGAr",
        "outputId": "788e8e15-d71a-4f3d-99e8-30d878c68b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4776, 0.5224, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2985, 0.3569, 0.3446, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2360, 0.2568, 0.2508, 0.2564, 0.0000, 0.0000],\n",
            "        [0.1939, 0.2045, 0.2032, 0.2054, 0.1930, 0.0000],\n",
            "        [0.1503, 0.1790, 0.1720, 0.1794, 0.1494, 0.1699]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-XqXxKcYjbe"
      },
      "source": [
        "### Masking additional attention weights with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGyeNovHe97q",
        "outputId": "02a315e6-8f37-4a01-ae3d-4ad0ceabd208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n",
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6893, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5136, 0.0000, 0.5128, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4091, 0.4064, 0.4108, 0.3859, 0.0000],\n",
            "        [0.3005, 0.3580, 0.0000, 0.0000, 0.2988, 0.3398]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6, 6) # create a matrix of ones\n",
        "\n",
        "print(dropout(example))\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9schnK0fRte"
      },
      "source": [
        "### Causal Attention Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "S8GxHW4IfRVU"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        print(b)\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eoq6pVwfyzu",
        "outputId": "60baf7a1-d4b5-4cc4-b32a-ecbb19bad567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "2\n",
            "tensor([[[-0.2810, -0.1618],\n",
            "         [-0.3837, -0.1213],\n",
            "         [-0.5832, -0.2144],\n",
            "         [-0.5292, -0.1581],\n",
            "         [-0.4817, -0.1670],\n",
            "         [-0.5365, -0.1788]],\n",
            "\n",
            "        [[-0.2810, -0.1618],\n",
            "         [-0.3837, -0.1213],\n",
            "         [-0.5832, -0.2144],\n",
            "         [-0.5292, -0.1581],\n",
            "         [-0.4817, -0.1670],\n",
            "         [-0.5365, -0.1788]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = torch.stack((rounded_embeddings, rounded_embeddings), dim=0)\n",
        "print(batch.shape) # 2 batches with 6 tokens each, and each token has embedding dimension 3\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UydLlF4rj0hf"
      },
      "source": [
        "## Self Attention multi-head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0f3nXA9ne-Q"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAZxS6r3nWzN",
        "outputId": "632c3ae1-b538-492a-8762-17da79804c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "torch.Size([2, 6, 2, 1]) vs torch.Size([2, 2, 6, 1])\n",
            "--------------------\n",
            "tensor([[[0.2035, 0.5207],\n",
            "         [0.2297, 0.4747],\n",
            "         [0.2453, 0.3592],\n",
            "         [0.2474, 0.3960],\n",
            "         [0.2365, 0.4187],\n",
            "         [0.2444, 0.3881]],\n",
            "\n",
            "        [[0.2035, 0.5207],\n",
            "         [0.2297, 0.4747],\n",
            "         [0.2453, 0.3592],\n",
            "         [0.2474, 0.3960],\n",
            "         [0.2365, 0.4187],\n",
            "         [0.2444, 0.3881]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "# Multiple heads to extract differente type of information, every head is using different initialized weights\n",
        "# cada head da como resultado context vectors de cierte dimension que al final son concatenados\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape #b is for batches\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        print('--------------------')\n",
        "        print(f'{keys.shape} vs {keys.transpose(1,2).shape}')\n",
        "        print('--------------------')\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "123g1w8Vz_0T",
        "KiqznPKxE2-J",
        "kQyY8us8kjnU",
        "sdqVKwHw9WOr",
        "e5oaJW523vt2",
        "b0nYzc5uBMkF",
        "cXjmjDeM6z-5",
        "t-qc5coF7P9c",
        "scsHf8OgJiSX",
        "N3bdp26B93oU",
        "JGLUALUt968u",
        "SC3hdDBCQyxj",
        "4gYidQzDQAI5",
        "UryRjq28FZ39"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}